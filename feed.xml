<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="http://www.micahlerner.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://www.micahlerner.com/" rel="alternate" type="text/html" /><updated>2021-09-06T22:44:53-07:00</updated><id>http://www.micahlerner.com/feed.xml</id><title type="html">www.micahlerner.com</title><author><name>Micah</name></author><entry><title type="html">A Linux Kernel Implementation of the Homa Transport Protocol, Part II</title><link href="http://www.micahlerner.com/2021/08/29/a-linux-kernel-implementation-of-the-homa-transport-protocol.html" rel="alternate" type="text/html" title="A Linux Kernel Implementation of the Homa Transport Protocol, Part II" /><published>2021-08-29T00:00:00-07:00</published><updated>2021-08-29T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/29/a-linux-kernel-implementation-of-the-homa-transport-protocol</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/29/a-linux-kernel-implementation-of-the-homa-transport-protocol.html">&lt;p&gt;&lt;em&gt;Programming note: I will be taking a several week break from writing paper reviews for the summer. When we come back, I will be finishing off the papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt;, then moving on to the great upcoming conferences (my non-exhaustive list is &lt;a href=&quot;https://www.micahlerner.com/2021/08/14/systems-conferences-2021.html&quot;&gt;here&lt;/a&gt;). As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These paper reviews can &lt;a href=&quot;https://tinyletter.com/micahlerner/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the new &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/atc21-ousterhout.pdf&quot;&gt;A Linux Kernel Implementation of the Homa Transport Protocol&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review is Part II in a series on the Homa Transport Protocol - part I is available &lt;a href=&quot;/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html&quot;&gt;here&lt;/a&gt;. As a refresher, Homa is a transport protocol with the goal of replacing TCP in the data center. The first part of the series focuses on describing the goals of Homa, while this paper review discusses an &lt;a href=&quot;https://github.com/PlatformLab/HomaModule&quot;&gt;open source implementation&lt;/a&gt; of the protocol as a Linux Kernel module&lt;label for=&quot;linuxkernelmodule&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;linuxkernelmodule&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There is an excellent Kernel Module programming &lt;a href=&quot;https://sysprog21.github.io/lkmpg/&quot;&gt;guide&lt;/a&gt; that has been revamped continuously since the 2.2 kernel. Another great description of writing your own Linux Kernel module &lt;a href=&quot;https://linux-kernel-labs.github.io/refs/heads/master/labs/kernel_modules.html&quot;&gt;here&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The author (John Ousterhout, one of the inventors of the &lt;a href=&quot;https://raft.github.io/&quot;&gt;Raft&lt;/a&gt; consensus algorithm) has three goals in mind with implementing Homa as a Linux Kernel Module:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understand how Homa performs in a more production-like environment, represented by the Linux kernel.&lt;/li&gt;
  &lt;li&gt;Perform apples to apples comparisons of Homa to implementations of competing protocols (TCP and DCTCP).&lt;/li&gt;
  &lt;li&gt;Build an implementation of Homa that could be used and extended by real users&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;In accomplishing the three goals above, the paper makes two contributions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Showing that Homa beats TCP and DCTCP&lt;label for=&quot;dctcp&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dctcp&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;DCTCP is a project that preceded Homa, but has a similar goal of replacing TCP in the dataceter. The paper is &lt;a href=&quot;https://people.csail.mit.edu/alizadeh/papers/dctcp-sigcomm10.pdf&quot;&gt;here&lt;/a&gt;. &lt;/span&gt;, replicating the results of the paper presented in &lt;a href=&quot;/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html&quot;&gt;Part I&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;An analysis of Homa’s limits. This study indicates potential future directions for research tackling the tension between faster network speeds and using more cores to handle increased bandwidth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;homa-api&quot;&gt;Homa API&lt;/h2&gt;

&lt;p&gt;Homa aims to deliver a connectionless transport protocol for RPCs in the data center. The protocol’s approach contrasts with TCP on two dimensions.&lt;/p&gt;

&lt;p&gt;Homa is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;RPC-oriented, rather than stream-oriented&lt;/em&gt;: TCP’s stream-based approach (which relies on FIFO delivery of messages) can experience high tail latency. The cause of this latency is head of line blocking, where delay in a message at the front (or “head”) of a stream delays the rest of the stream. Homa limits head of line blocking because the protocol does not enforce FIFO ordering of messages.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Connectionless, rather than connection-oriented&lt;/em&gt;: TCP’s connection-oriented approach is not well suited for datacenters because “applications can have hundreds or thousands of them, resulting in high space and time overheads”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make the protocol available to developers, the implementation defines an API focused on sending and receiving RPC messages. The primary methods in the API are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homa_send&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homa_recv&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homa_reply&lt;/code&gt;. These calls operate on sockets that can be reused for many different RPC requests (notably different from TCP). The methods return or accept a 64 bit identifier for a corresponding RPC. Furthermore, an RPC-based approach facilitates abstracting away logic responsible for Homa’s reliability, like the internals of retries.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa2/api.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;challenges-in-implementing&quot;&gt;Challenges in implementing&lt;/h2&gt;

&lt;p&gt;The paper outlines three main challenges to&lt;label for=&quot;highspeed&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;highspeed&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;When viewing these challenges, it is important to remember that Homa is designed for reliable high-speed datacenter networks. Thus the constraints Homa faces in the kernel are different than in other, non-datacenter environments. &lt;/span&gt; implementing Homa as a Linux Kernel module:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Moving packets through the protocol stack is costly.&lt;/li&gt;
  &lt;li&gt;Multiple cores are needed to process incoming packets, yet Linux load balancing of this work is non-optimal.&lt;/li&gt;
  &lt;li&gt;Packets need to be assigned priorities and transmitted in real-time. Accomplishing this task with a single core is difficult, using multiple cores to solve the problem even more so.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sending packets is costly, as doing so involves copies and interaction with other Linux features. One approach to this overhead is userspace networking&lt;label for=&quot;zerocopy&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;zerocopy&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Touched on in a past paper review of &lt;a href=&quot;https://www.micahlerner.com/2021/07/07/breakfast-of-champions-towards-zero-copy-serialization-with-nic-scatter-gather.html&quot;&gt;Breakfast of Champions: Towards Zero-Copy Serialization with NIC Scatter-Gather&lt;/a&gt;. &lt;/span&gt;. Another approach mentioned in the paper is batching packets together to amoritize cost - unfortunately, this approach does not work well for Homa because batching packets can introduce latency (a main concern of the protocol).&lt;/p&gt;

&lt;p&gt;Multiple cores are needed to process packets because networks are improving faster than CPUs are&lt;label for=&quot;killermicroseconds&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;killermicroseconds&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;“A problem discussed in &lt;a href=&quot;https://cacm.acm.org/magazines/2017/4/215032-attack-of-the-killer-microseconds/fulltext&quot;&gt;this&lt;/a&gt; great ACM article on “Attack of the Killer Microseconds”. &lt;/span&gt;. A challenge to using multiple cores is Linux scheduling, which creates “software congestion” when “too much work is assigned to one core”.&lt;/p&gt;

&lt;p&gt;Lastly, Homa strives to assign priorities to packets, while minimizing the size of the network interface card’s (NIC) transmit queue - more items in this queue means a potentially longer wait time, and more tail latency.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;As discussed above, there are three primary challenges to implementing Homa as a Linux Kernel module. These challenges impact the sending and receiving path for packets - the visualization below describes these two paths and the components involved in implementing them. Fair warning that the implementation is heavy on Linux internals, and I try to link to documentation for further deep dives where possible!&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa2/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;moving-packets&quot;&gt;Moving packets&lt;/h3&gt;

&lt;p&gt;The first challenge in implementing Homa is the cost of moving packets through the networking stack. To solve this problem, the implementation uses batching on the send and receive paths, rather than pushing packets through the stack one by one.&lt;/p&gt;

&lt;p&gt;On the sending path, Homa/Linux uses TCP Segmentation Offload (TSO)&lt;label for=&quot;tso&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;tso&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are extensive docs on Linux segmentation offloading &lt;a href=&quot;https://www.kernel.org/doc/Documentation/networking/segmentation-offloads.txt&quot;&gt;here&lt;/a&gt;. Generic Segment Offload is also mentioned, but isn’t supported at this time. &lt;/span&gt;. A TSO-based strategy offloads work to the NIC - the kernel passes large packets to the NIC, which then performs the work of breaking down the packet into smaller segments.&lt;/p&gt;

&lt;p&gt;The implementation of batching on the receive path is somewhat more complicated. When the NIC receives packets, it issues an interrupt. In response to the interrupt, the networking driver schedules a &lt;em&gt;NAPI&lt;/em&gt;&lt;label for=&quot;napi&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;napi&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;NAPI stands for “New API” and is a framework for packet processing. I found this additional documentation on the &lt;a href=&quot;https://wiki.linuxfoundation.org/networking/napi&quot;&gt;Linux Foundation site&lt;/a&gt; useful. &lt;/span&gt; action that polls the NIC for packets until it reaches a configured limit. Once the driver reaches this limit, it communicates batches to the &lt;em&gt;SoftIRQ&lt;/em&gt; layer of the Linux kernel. &lt;em&gt;SoftIRQ&lt;/em&gt; “is meant to handle processing that is almost — but not quite — as important as the handling of hardware interrupts”.&lt;label for=&quot;softirq&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;softirq&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;From Jonathan Corbet’s &lt;a href=&quot;https://lwn.net/Articles/520076/&quot;&gt;2012 article&lt;/a&gt;. Another comprehensive article on receiving data in the Linux Networking Stack is &lt;a href=&quot;https://packagecloud.io/blog/illustrated-guide-monitoring-tuning-linux-networking-stack-receiving-data/&quot;&gt;here&lt;/a&gt;. &lt;/span&gt; Homa builds up messages from the incoming batches, and signals waiting application threads once a message is complete - these applications are then able to make use of the response to the Homa calls mentioned in the API section above.&lt;/p&gt;

&lt;h3 id=&quot;load-balancing&quot;&gt;Load balancing&lt;/h3&gt;

&lt;p&gt;Homa is intended for high speed networks under load. In this environment, a single core is not capable of processing incoming packets - to use multiple cores, Homa must load balance work&lt;label for=&quot;outgoing&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;outgoing&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper notes that, “load balancing is easy for packet output, because the output stack executes entirely on the sending thread’s core, with a separate NIC channel per core. The Linux scheduler balances threads across cores, and this distributes the packet transmission load as well.” &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Load balancing is implemented in the kernel with two load balancers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Receive Side Scaling (RSS), which performs load balancing inside the NIC to distribute processing across CPUs. The Linux Networking documentation provides &lt;a href=&quot;https://www.kernel.org/doc/Documentation/networking/scaling.txt&quot;&gt;helpful documentation&lt;/a&gt; on RSS.&lt;/li&gt;
  &lt;li&gt;NAPI (mentioned previously), which performs load balancing at the SoftIRQ layer (once batches of packets are created, those batches need to communicated to waiting application threads)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper also mentions that the balancing implementation hurts performance at low load, as “at low load it is best to concentrate all processing on a single core”&lt;label for=&quot;effect&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;effect&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The experimental section of the paper quantifies this effect. &lt;/span&gt;.  While ideally Homa could implement an adaptive load balancing scheme, the paper mentions that “there does not appear to be a way to do this in Linux.” This remark ties into a theme throughout the paper - that the Linux kernel’s focus on TCP (in particular, design impacted by this focus) introduces overhead.&lt;/p&gt;

&lt;h3 id=&quot;real-time-processing&quot;&gt;Real-time processing&lt;/h3&gt;

&lt;p&gt;Homa aims to assign packet priorities and limit the amount of time packets spend in the NIC’s transmit queue - more time in the transmit queue means more delay/potential tail latency. Because the NICs used do not make the size of their transmit queues available, Homa needs to estimate their size. The implementation does so using an estimate of the size of the packets and the link speed. This estimate is updated by a &lt;em&gt;pacer&lt;/em&gt; thread (visible in the architecture diagram above). Unfortunately, there are complications to running the &lt;em&gt;pacer&lt;/em&gt; thread: the pacer can not keep up at high bandwidth, and the operating system scheduler potentially interferes by descheduling the thread’s execution. The paper outlines three workarounds that assist the pacer thread, ensuring it doesn’t fall behind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Small packets don’t interact with the pacer (meaning less work)&lt;/li&gt;
  &lt;li&gt;Other cores pitch in if the main pacer thread falls behind&lt;/li&gt;
  &lt;li&gt;Other parts of the Homa implementation will queue packets if the thread falls behind&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;A primary goal of the paper was to evaluate Homa in a production-like environment, reproducing the results of the original Homa paper (covered in &lt;a href=&quot;/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html&quot;&gt;Part I&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;To accomplish this goal, the paper tests the Linux implementation of Homa with four workloads from the original paper. The workloads cover a wide arrange of message sizes (including both small and large RPCs). Furthermore, the paper focuses on cases where there are many clients - Homa is not well suited for situations where there are few RPC clients (arguing that this situation does not arise in data center like environments). The same workloads are executed with TCP and DCTCP (a TCP-like protocol adapted for the datacenter), and compared to Homa’s results.&lt;/p&gt;

&lt;p&gt;The key metric used in this set of performance evaluations is &lt;em&gt;slowdown&lt;/em&gt;. &lt;em&gt;Slowdown&lt;/em&gt; is calculated by comparing the round trip time (RTT) of an RPC to the RTT observed using Homa under ideal conditions (Homa is designed to perform well for small messages on a network under high load). Smaller values of slowdown are better than larger values - larger values for slowdown mean that the result is significantly worse than one would expect from Homa under ideal conditions.&lt;/p&gt;

&lt;p&gt;The graphs below show Homa’s significantly lower slowdown relative to TCP and DCTCP for a variety of message sizes.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa2/workloads.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The paper also includes a number of microbenchmarks focused on validating other aspects of the implementation, like how well Homa performs with different numbers of prioritiy levels, or how well the implementation performs under reduced load&lt;label for=&quot;lowload&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;lowload&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Homa is designed for high load, so it is useful to evaluate the implementation in situations it might not otherwise perform well under. &lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conclusion of the Homa paper asserts that while the implementation “eliminates congestion as a significant performance factor”, remaining software-based overheads pose a future area of improvement. These overheads come from conflicts between Homa and Linux implementation details (like scheduling and load balancing that optimize for TCP).&lt;/p&gt;

&lt;p&gt;The paper discusses two potential solutions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Moving transport protocols to user space&lt;label for=&quot;userspace&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;userspace&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;An interesting topic that will be covered in papers from at least one conference paper over the next few months! &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;Moving transport protocols to the NIC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I thoroughly enjoyed diving into Homa - stay tuned for when we resume in the next few weeks. When we will cover papers from OSDI, ATC, and the upcoming set of conferences. Until then!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Programming note: I will be taking a several week break from writing paper reviews for the summer. When we come back, I will be finishing off the papers from Usenix ATC and OSDI, then moving on to the great upcoming conferences (my non-exhaustive list is here). As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These paper reviews can be delivered weekly to your inbox, or you can subscribe to the new Atom feed.</summary></entry><entry><title type="html">Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities, Part I</title><link href="http://www.micahlerner.com/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html" rel="alternate" type="text/html" title="Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities, Part I" /><published>2021-08-15T00:00:00-07:00</published><updated>2021-08-15T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/15/a-linux-kernel-implementation-of-the-homa-transport-protocol.html">&lt;p&gt;&lt;em&gt;Over the next few weeks I will be reading papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt; - as always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These weekly paper reviews can &lt;a href=&quot;https://tinyletter.com/micahlerner/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the new &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://people.csail.mit.edu/alizadeh/papers/homa-sigcomm18.pdf&quot;&gt;Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities&lt;/a&gt;, Montazeri et al., SIGCOMM 2018&lt;/p&gt;

&lt;p class=&quot;discussion&quot;&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=28204808&quot;&gt; Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review is part one of a two-part series on the same research topic - &lt;em&gt;Homa&lt;/em&gt;, a transport protocol purpose-built to replace TCP for low-latency RPC inside the modern data center&lt;label for=&quot;kurose&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;kurose&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;If you are interested in learning more about networking, I can’t recommend the Kurose &amp;amp; Ross book enough. Although it isn’t free, there &lt;em&gt;is&lt;/em&gt; a large amount of course content (like videos), on &lt;a href=&quot;https://gaia.cs.umass.edu/kurose_ross/online_lectures.htm&quot;&gt;their site&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Specifically, &lt;em&gt;Homa&lt;/em&gt; aims to replace TCP, which was designed in the era before modern data center environments existed. Consequently, TCP doesn’t take into account the unique properties of data center networks (like high-speed, high-reliability, and low-latency). Furthemore, the nature of RPC traffic is different - RPC communication in a data center often involve enormous amounts of small messages and communication between many different machines. TCP is non-ideal for this type of communication for several reasons - for example, it is designed to ensure reliable transmission of packets (under the assumption that the networks are not reliable), and is a connection-oriented protocol that requires state (meaning that operating many connections at once has a high overhead). The difference between the design goals of TCP and the nature of the data center leads to non-optimal performance under load, which shows up as tail latency&lt;label for=&quot;tail&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;tail&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For more on tail latency, I’d recommend reading &lt;a href=&quot;https://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/fulltext&quot;&gt;The Tail at Scale&lt;/a&gt; - there are also several great reviews of the paper (&lt;a href=&quot;https://blog.acolyer.org/2015/01/15/the-tail-at-scale/&quot;&gt;The Morning Paper&lt;/a&gt;, &lt;a href=&quot;https://squidarth.com/article/systems/2020/02/29/tail-at-scale.html&quot;&gt;Sid Shanker’s blog&lt;/a&gt;, or &lt;a href=&quot;https://www.youtube.com/watch?v=1Qxnrf2pW10&quot;&gt;Vivek Haldar’s video review&lt;/a&gt;). &lt;/span&gt;. To address this issue, researchers and industry created a number of solutions&lt;label for=&quot;newtransport&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;newtransport&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper cites &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/1851182.1851192&quot;&gt;DCTCP&lt;/a&gt;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2534169.2486031&quot;&gt;pFabric&lt;/a&gt;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3098822.3098825&quot;&gt;NDP&lt;/a&gt;, and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/2716281.2836086&quot;&gt;pHost&lt;/a&gt;. &lt;/span&gt; purpose built for the data center, of which Homa was the newest.&lt;/p&gt;

&lt;p&gt;I will be publishing this paper review in two parts. The first part gives an overview of the &lt;em&gt;Homa&lt;/em&gt; protocol, based on &lt;em&gt;Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities&lt;/em&gt; from SIGCOMM 2018. This paper lays out the problems that the research area is trying to solve, designs a solution to those problems, and presents experimental results. The second paper, &lt;em&gt;A Linux Kernel Implementation of the Homa Transport Protocol&lt;/em&gt; was published at this year’s USENIX ATC conference. It discusses the implementation (and challenges to implementing) Homa as a Linux Kernel module, with the goal of evaluating the protocol in a setting that is closer to a real production environment - this paper’s conclusion also discusses the limits of the implementation and a few exciting potential directions for future research.&lt;/p&gt;

&lt;p&gt;With that, let’s dive into understanding Homa.&lt;/p&gt;

&lt;h2 id=&quot;the-homa-protocol&quot;&gt;The Homa protocol&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;As discussed above, the problem that &lt;em&gt;Homa&lt;/em&gt; is trying to solve is a disconnect between the design of TCP and the unique qualities of data center networks. This disconnect increases latency and overhead of RPC communications, meaning wasted data center resources. Thus, &lt;em&gt;Homa&lt;/em&gt; is designed with the goal of achieving the “lowest possible latency” for RPC (in particular focusing on small messages at “high network load”).&lt;/p&gt;

&lt;p&gt;To achieve this goal, &lt;em&gt;Homa&lt;/em&gt; must consider a primary source of latency in this type of network: &lt;em&gt;queuing delay&lt;/em&gt;. Queuing delay occurs at routers in a network when more packets arrive than can be transmitted at once&lt;label for=&quot;queue&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;queue&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For an in depth discussion of delays, I recommend &lt;a href=&quot;https://archive.is/20130114163812/http://59.67.152.66:8000/newenglish/delay.htm&quot;&gt;this&lt;/a&gt; chapter from the Kurose and Ross networking book. &lt;/span&gt; (meaning that they need to wait in a queue). More queuing leads to more latency!&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/delay.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;Queuing delay, sourced from &lt;a href=&quot;http://www.cs.toronto.edu/~marbach/COURSES/CSC358_F19/delay.pdf&quot;&gt;here&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To limit queuing, a design could aim to eliminate it entirely or could accept that queueing will happen (while aiming to minimize its negative impact). The paper mentions one system, &lt;a href=&quot;TODO&quot;&gt;FastPass&lt;/a&gt;, that implements the first approach using a central scheduler that could theoretically optimally make packet-scheduling decisions. Unfortunately, interacting with the scheduler for every packet “triples the latency” for short messages.&lt;/p&gt;

&lt;p&gt;If queuing is accepted as inherent to the network, the paper argues &lt;em&gt;in-network priorities&lt;/em&gt; must be used to provide finer grained control over how packets are queued&lt;label for=&quot;pfabricq&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;pfabricq&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper mentions that previous work demonstrates the positive impact of using these types of priorities. &lt;/span&gt;. &lt;em&gt;In-network priorities&lt;/em&gt; allow a priority to be assigned to a packet, then for that packet to be assigned to a queue that contains only packets with that priority. This ensures that the highest priority packets are transmitted first and provides a degree of control over how different types of traffic is treated.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/priorityq.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;A depiction of a priority queue scheduling system sourced from &lt;a href=&quot;http://www2.ic.uff.br/~michael/kr1999/6-multimedia/6_06-scheduling_and_policing.htm&quot;&gt;this&lt;/a&gt; resource on packet scheduling.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To determine the priority of a packet, &lt;em&gt;Homa&lt;/em&gt; uses a policy called &lt;em&gt;Shortest Remaining Processing Time first&lt;/em&gt; (SRPT), “which prioritizes packets from messages with the fewest bytes remaining to transmit”. Another way of explaining SRPT is that it aims to schedule packets based on how close the RPC is to completing the transfer of its associated packets. If a packet is associated with an RPC request that has fewer packets left to transmit to a receiver, scheduling that packet first will allow the request to finish faster. The paper mentions that &lt;em&gt;SRPT&lt;/em&gt; is not only common in previous work (like &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2486001.2486031&quot;&gt;pFabric&lt;/a&gt;), but is close to optimal in the conditions that one would see in a network under load.&lt;/p&gt;

&lt;p&gt;Lastly, the paper discusses which parts of the system (client or receiver) should make decisions about the priority of a packet (by appling the SRPT policy) and when. The paper argues that receivers are well positioned to determine packet priorities - they know which clients are sending packets to them and could be configured to keep track of how much data each client has left to send. Even though receivers calculate packet priorities, clients also need to apply SRPT to the packets that they send out (if a client is sending multiple RPCs at once, the RPC that is closest to finishing should have its associated packets sent out first).&lt;/p&gt;

&lt;p&gt;Receivers are also in a position to optimize packet priorities beyond applying the SRPT policy. An example of further optimization is a process called &lt;em&gt;overcommitting&lt;/em&gt;, where the receiver instructs more than one sender to use the same priority at the same time to ensure full network utilization. As mentioned previously, a client might receive information about how to send out packets with optimal priorities, but might delay actually sending out the packets for some reason. One example of this is if a client is sending out multiple RPCs at once and the prioritized packets are delayed client-side while a different RPC is sent out.&lt;/p&gt;

&lt;h3 id=&quot;design-and-implementation&quot;&gt;Design and Implementation&lt;/h3&gt;

&lt;p&gt;Homa is implemented with the concerns above in mind, using receiver-driven priorities decided with the SRPT policy. To accomplish its goals, the system uses four packet types to send data, communicate priorities from receiver to sender, or signal metadata.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/packets.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When a client wants to send an RPC to a receiver, it sends an initial chunk and metadata that includes the total size of the message (which the receiver will use to track request completion). This chunk is given an &lt;em&gt;unscheduled&lt;/em&gt; priority (as seen in the system diagram below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/homa/protocol.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The receiver then applies the SRTP algorithm to decide the priority for the next set of packets associated with the given RPC, then communicates the priority back to the sender using a &lt;em&gt;GRANT&lt;/em&gt; packet. The &lt;em&gt;GRANT&lt;/em&gt; packet instructs the sender to send a configurable number of bytes (called &lt;em&gt;RTT Bytes&lt;/em&gt;) before waiting for another grant. Once the sender receives this information, it sends packets using the &lt;em&gt;scheduled&lt;/em&gt; priority until it reaches the configured limit set via &lt;em&gt;RTT Bytes&lt;/em&gt; (the paper uses 10 KB, but mentions that this number will continue to grow as link speed increases).&lt;/p&gt;

&lt;p&gt;Now that we understand the basics of Homa, it is interesting to contrast the protocol with TCP. Homa forgoes features of TCP (and other RPC systems) that increase overhead and latency:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Explicit acknowledgements&lt;/em&gt;: senders transmit many packets without requiring acknowledgement, occasionally waiting for feedback from the receiver (who provides feedback via &lt;em&gt;GRANT&lt;/em&gt; packets). This approach means fewer packets need to be transmitted as part of the protocol, meaning more bandwidth can be dedicated to transmitting packets that contain RPC data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Connections&lt;/em&gt;: Homa is connectionless, unlike TCP. Foregoing connections means that Homa does not need to maintain certain types of state, like TCP does. Lower state overhead means Homa is able to service many more RPCs than a TCP-based sender-receiver pair would. Relatedly, the state that Homa maintains is bounded by the RTT bytes configuration parameter - there is a limit to how much data will be transmitted by a sender before waiting for feedback (and a limit to associated data that a single RPC request will consume in the router’s buffers).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;At-most-once delivery semantics&lt;/em&gt;: Other RPC protocols are designed to ensure at-most once-delivery of a complete message, but Homa targets &lt;em&gt;at-least-once&lt;/em&gt; semantics&lt;label for=&quot;atmostonce&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;atmostonce&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;While &lt;a href=&quot;https://www.lightbend.com/blog/how-akka-works-at-least-once-message-delivery&quot;&gt;this guide&lt;/a&gt; focuses on Akka, it is a helpful overview of the different messaging semantics. &lt;/span&gt;. This means that Homa can possibly re-execute RPC requests if there are failures in the network (and an RPC ends up being retried). While at-least-once semantics put a greater burden on the receiving system (which might have to make RPCs idempotent), relaxing the messaging semantics allows Homa receivers to adapt to failures that happen in a data center environment. As an example, Homa receivers can discard state if an RPC becomes inactive, which might happen if a client exceeds a deadline and retries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The original Homa paper discusses testing the protocol on a variety of workloads - the most recent paper on Homa (covered next week) includes a Linux-compatible implementation and aims to reproduce the evaluation of the protocol in a setting that is closer to one used in production. If you enjoyed this paper review, stay tuned for the next in the series!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Over the next few weeks I will be reading papers from Usenix ATC and OSDI - as always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These weekly paper reviews can be delivered weekly to your inbox, or you can subscribe to the new Atom feed.</summary></entry><entry><title type="html">Systems Conferences 2021</title><link href="http://www.micahlerner.com/2021/08/14/systems-conferences-2021.html" rel="alternate" type="text/html" title="Systems Conferences 2021" /><published>2021-08-14T00:00:00-07:00</published><updated>2021-08-14T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/14/systems-conferences-2021</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/14/systems-conferences-2021.html">&lt;p&gt;In an effort to keep track of conferences that I want to read papers from, I put together an incomplete list. The list is certainly non-exhaustive - feel free to send a pull request with suggestions!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Conference&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;http://cidrdb.org/cidr2021/index.html&quot;&gt;Conference on Innovative Data Systems Research (CIDR)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;January 11-15, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/fast21&quot;&gt;File and Storage Technologies (FAST)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;February 23 – 25, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/nsdi21&quot;&gt;USENIX Symposium on Networked Systems Design and Implementation (NSDI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;April 12–14, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.eurosys.org/&quot;&gt;EuroSys 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;April 26—28, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sigops.org/s/conferences/hotos/2021/&quot;&gt;HotOS&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;June 1 - 3, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.sigmod.org/&quot;&gt;SIGMOD/PODS&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;June 20 - 25, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.debs.org/full-program/&quot;&gt;DISTRIBUTED AND EVENT-BASED SYSTEMS (DEBS) 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;June 28 - July 2, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;USENIX Symposium on Operating Systems Design and Implementation (OSDI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;July 14–16, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;USENIX Annual Technical Conference&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;July 14–16, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.hotstorage.org/2021/&quot;&gt;HotStorage&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;July 27–28, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity21&quot;&gt;USENIX Security&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;August 11–13, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://vldb.org/2021/&quot;&gt;Very Large Data Base (VLDB)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;August 16-20, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://conferences.sigcomm.org/sigcomm/2021/&quot;&gt;SIGCOMM 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;August 23–27, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.usenix.org/conference/srecon21&quot;&gt;SRECon&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;October 12–14, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sosp2021.mpi-sws.org/&quot;&gt;Symposium on Operating Systems Principles (SOSP)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;October 25-28, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://2021.splashcon.org/&quot;&gt;Systems, Programming, Languages, and Applications: Software for Humanity (SPLASH)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;October 17-22, 2021&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://acmsocc.org/2021/&quot;&gt;ACM Symposium on Cloud Computing 2021&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;November 1-3, 2021&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Micah</name></author><summary type="html">In an effort to keep track of conferences that I want to read papers from, I put together an incomplete list. The list is certainly non-exhaustive - feel free to send a pull request with suggestions!</summary></entry><entry><title type="html">POSH: A Data-Aware Shell</title><link href="http://www.micahlerner.com/2021/08/07/posh-a-data-aware-shell.html" rel="alternate" type="text/html" title="POSH: A Data-Aware Shell" /><published>2021-08-07T00:00:00-07:00</published><updated>2021-08-07T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/08/07/posh-a-data-aware-shell</id><content type="html" xml:base="http://www.micahlerner.com/2021/08/07/posh-a-data-aware-shell.html">&lt;p&gt;&lt;em&gt;This is the fourth paper in a series on “The Future of the Shell”&lt;label for=&quot;series&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;series&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Here are links to &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html&quot;&gt;Part 2&lt;/a&gt;, and &lt;a href=&quot;/2021/07/31/pash-light-touch-data-parallel-shell-processing.html&quot;&gt;Part 3&lt;/a&gt;. &lt;/span&gt;. These weekly paper reviews can &lt;a href=&quot;https://tinyletter.com/micahlerner/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, or you can subscribe to the new &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt;. Over the next few weeks I will be reading papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt; - as always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/conference/atc20/presentation/raghavan&quot;&gt;POSH: A Data-Aware Shell&lt;/a&gt; Deepti Raghavan, et. al&lt;/p&gt;

&lt;p class=&quot;discussion&quot;&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=28108347&quot;&gt; Hacker News&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper review covers &lt;em&gt;POSH&lt;/em&gt;, a system capable of achieving dramatic speedups for &lt;em&gt;unmodified&lt;/em&gt;&lt;label for=&quot;adoption&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;adoption&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Requiring fewer changes to a shell script in order to make it POSH-compatible simplifies adoption. &lt;/span&gt; shell scripts that perform large amounts of IO - intriguing use cases of POSH are log analysis or the git workflows of large software projects&lt;label for=&quot;git&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;git&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper analyzes git workflows for &lt;a href=&quot;https://github.com/chromium/chromium&quot;&gt;Chromium&lt;/a&gt;. &lt;/span&gt;. In particular, POSH shines in environments that use distributed file systems like Network File System&lt;label for=&quot;nfs&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;nfs&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;NFS allows you to “mount” a remote disk to your computer and then sends RPC calls to a remote server to perform file accesses. I highly recommend &lt;a href=&quot;https://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf&quot;&gt;this&lt;/a&gt; amazing (and free!) description of NFS from &lt;em&gt;Operating Systems: Three Easy Pieces&lt;/em&gt;. The entire book is available for free online &lt;a href=&quot;https://pages.cs.wisc.edu/~remzi/OSTEP/&quot;&gt;here&lt;/a&gt;.  &lt;/span&gt; (NFS) mounts - I’ve included a link to a great overview of NFS in the sidebar (or if you are on mobile, you can click the number “4” to reveal it).&lt;/p&gt;

&lt;p&gt;POSH achieves speedups by minimizing data transfers in scripts that use networked storage. To minimize data transfers, POSH can execute parts of a script that read or write remote files in a process on the remote machine. As an example, consider a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; of a file stored on a remote machine. When a client&lt;label for=&quot;client&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;client&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Client in this case meaning the computer where the script was initiated by a user. &lt;/span&gt; computer attempts to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; the file, the shell will transfer &lt;em&gt;the whole file&lt;/em&gt; over the network to the client node, then filter the file on the client. In contrast, POSH can perform the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; on the remote storage server, and only transfer the filtered output back to the client, dramatically lowering network traffic.&lt;/p&gt;

&lt;p&gt;To make decisions about which parts of a script are executed remotely, POSH produces a graph representation of the shell script’s execution - the nodes in the graph are commands, while the edges represent the flow of data between commands. Correctly transforming a shell script into this graph representation is a nuanced, yet critical function. To facilitate it, POSH leverages an annotation language capable of describing a given command’s parameters, inputs, and outputs (as well as a number of important configuration options).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;POSH&lt;/em&gt; and the system described in &lt;a href=&quot;/2021/07/31/pash-light-touch-data-parallel-shell-processing.html&quot;&gt;last week’s paper review&lt;/a&gt;, &lt;em&gt;PaSh&lt;/em&gt;, are similar in that they both aim to speedup shell script execution without requiring modifications to the original script. Additionally, they both leverage annotations of shell commands in their implementations. Even though the two projects are similar in some respects, PaSh and POSH focuses on different uses cases - PaSH focuses on parallelizing “trivially parallelizable” computation local to a machine, while POSH focuses on parallelizing scripts that perform large amounts of IO across remote machines. Both projects are part of an exciting (and high impact) thread of research related to modernizing the shell, and I’m looking forward to seeing more from the two teams!&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper makes two contributions. The first is an &lt;em&gt;annotation language&lt;/em&gt; that describes a shell command. These command specifications are used to transform the script into a graph representation - the different steps of a script’s execution are the nodes, and the data flow between those nodes are the edges. The second contribution is a &lt;em&gt;scheduling algorithm&lt;/em&gt; that decides how the steps in a script should be executed, taking into account the dependencies in the script’s graph representation as well as the interactions that a step has with remote storage.&lt;/p&gt;

&lt;p&gt;Before we dive into the details of these two contributions, it is first helpful to understand POSH’s three high level components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Annotation interface&lt;/em&gt;: As mentioned above, the annotation language allows a shell script to be correctly transformed into a graph representation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Parser and scheduler&lt;/em&gt;: The parser uses the aforementioned annotations to produce a graph representation of a shell script. The scheduler uses this graph representation to assign the execution of steps to remote or local nodes called &lt;em&gt;proxy servers&lt;/em&gt;. The internals of the scheduling process are detailed later on in this paper review.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Execution engine&lt;/em&gt;: Once the scheduler has assigned work to a &lt;em&gt;proxy server&lt;/em&gt;, that work will be executed, and the result will be transferred over the network back to the client node.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/posh/system.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;High-level POSH overview&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;shell-annotation-language&quot;&gt;Shell annotation language&lt;/h2&gt;

&lt;p&gt;POSH uses its shell annotation language to describe the constraints of any given shell command’s execution. These annotations are then used to transform a shell script into a correct graph representation that, when scheduled, will accomplish POSH’s goal of minimizing network traffic.&lt;/p&gt;

&lt;p&gt;The paper outlines three questions that POSH (and the annotation language) must answer to achieve the system’s goals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Which commands can be executed on remote nodes (called proxy servers)?&lt;/em&gt;: this is important for determining what must run locally, versus what can run on a remote &lt;em&gt;proxy server&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Do any commands in a provided script “filter their input”?&lt;/em&gt;: knowing if a command does or does not filter its input is useful for determining whether it should be executed remotely in conjunction with other commands. The paper provides the example of a executing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; followed by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; on the same remote &lt;em&gt;proxy server&lt;/em&gt; - as “cat usually produces the same amount of output as input, but grep usually filters its input, POSH must also offload grep” to minimize network traffic.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Can a command be parallelized?&lt;/em&gt;: to enable optimal scheduling, POSH should aim to parallelize a command as much as possible. Without an annotation language, the system might not be have the information it needs to make scheduling decisions. One motivating example is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file1 file2 file3&lt;/code&gt; - the annotation language defines that the inputs to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; are “splittable”, meaning that it might be possible run the three commands &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file2&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat file3&lt;/code&gt; in parallel on different machines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wanted to note two important components of the annotation language important to understanding the rest of the paper&lt;label for=&quot;opt&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;opt&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper provides a significant amount of detail on the annotation language and I highly recommend referring to the original paper if this is interesting to you! &lt;/span&gt;. First, the annotations can be defined per command and per argument - this flexibility is important because different arguments to a command can change its behavior and arguments. Second, a command’s inputs/outputs can be typed, and its behavior is defined. For example, the annotation language can indicate a command’s parallelizablity&lt;label for=&quot;p&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;p&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;As an example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; is annotated with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;splittable&lt;/code&gt; to indicate that it is potentially parallelizable. &lt;/span&gt; or whether the command relies on the current directory&lt;label for=&quot;git&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;git&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;An example being &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git add&lt;/code&gt; is dependent on the current directory. &lt;/span&gt;. Defining these properties of a command allow the parser and scheduler to answer the three questions above.&lt;/p&gt;

&lt;p&gt;The next section covers how a graph representation of a shell script, produced by passing the shell script through POSH’s parser, is scheduled and executed.&lt;/p&gt;

&lt;h2 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h2&gt;

&lt;p&gt;As discussed above, each shell script is passed through the POSH parser to produce a graph representation. The nodes in the graph representation are then scheduled to execute based on a two step process that &lt;em&gt;resolves scheduling constraints&lt;/em&gt; and &lt;em&gt;minimizes network transfers&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/posh/dag.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The first step of scheduling, &lt;em&gt;resolving constraints&lt;/em&gt;, determines whether any nodes in the graph &lt;em&gt;must&lt;/em&gt; run on a given remote machine (and if so, which one). Scheduling constraints are created for a variety of reasons - one example constraint is for a command that accesses remote files. To avoid transferring the whole file over the network, that command &lt;em&gt;must&lt;/em&gt; be scheduled on the remote node.&lt;/p&gt;

&lt;p&gt;The second step, &lt;em&gt;minimizing data transfer&lt;/em&gt;, assigns commands to a remote machine if the command was not assigned in the first step. For this assignment, POSH makes use of some graph theory and implements an algorithm using &lt;em&gt;sources&lt;/em&gt;, &lt;em&gt;sinks&lt;/em&gt;, and &lt;em&gt;paths&lt;/em&gt;&lt;label for=&quot;yegge&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;yegge&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;To quote Steve Yegge, “Graphs are, like, really really important.” &lt;/span&gt;. A &lt;em&gt;source&lt;/em&gt; is a “file that is read”, a &lt;em&gt;sink&lt;/em&gt; is the “output file that is written to”, and &lt;em&gt;paths&lt;/em&gt; connect them. To assign nodes, POSH iterates over every source node, checking whether the sink and source node in the path are already assigned to the same machine - if they are, assign all the intermediate nodes along the path to that machine as well! If the sink is not on the same machine, “the scheduler must find the edge along which cross-location data transfer should occur: to minimize data transfer, this should be the edge where the least data flows.” The paper describes a set of heuristics (&lt;a href=&quot;https://github.com/deeptir18/posh/blob/151b0729c4c45829485619c497506a264b0fea02/shell/src/scheduler/heuristic.rs#L37&quot;&gt;implemented here in Rust!&lt;/a&gt;) used to find the min-cut edge in the path. After this edge is found, unassigned nodes are scheduled to run on the machine that the source or sink is scheduled for, “depending on if the node is before or after the minimum cut edge”.&lt;/p&gt;

&lt;h2 id=&quot;applying-and-evaluating-posh&quot;&gt;Applying and evaluating POSH&lt;/h2&gt;

&lt;p&gt;POSH was evaluted on the time it takes to execute a number of applications. This paper review focuses on two specific applications: a distributed log analyis and a git workflow for Chromium. The experimental configuration involved using either a cloud-to-cloud setup (where client and machines are in the cloud) or a university-to-cloud setup (where the POSH client is located at Stanford). The cloud-to-cloud setup has significantly higher bandwidth and significantly lower RTT, and helps to demonstrate that POSH is capable of achieving speedups even with a more powerful network.&lt;/p&gt;

&lt;p&gt;The baseline performance measurement in these experiments comes from exercising each application using NFS instead of POSH. The NFS-only setup mimics a situation where the applications would perform IO-heavy workloads, but be unable to parallelize them (nor be able to limit network overhead).&lt;/p&gt;

&lt;p&gt;For the distributed log analysis (which involves searching for an IP address in a 15GB log dump), POSH sees a speedup from parallelizing across multiple NFS mounts in both experimental setups, although POSH sees a more dramatic speedup in the university-to-cloud setup than in the cloud-to-cloud setup (12.7x improvement in the former versus 2x improvement in the latter).&lt;/p&gt;

&lt;p&gt;For the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; workflow experiment, git operations (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git status&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git add&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git commit&lt;/code&gt;) were exercised by reverting, then recommitting 20 commits from the (quite large) Chromium open source project - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; commands on such a large project make many metadata calls (to determine whether a file has changed, for example). POSH shines in this experiment, achieving a 10-15x latency improvement in the cloud-to-cloud environment. This application seems incredibly useful - in the past, I’ve read about Facebook’s efforts to &lt;a href=&quot;https://engineering.fb.com/2014/01/07/core-data/scaling-mercurial-at-facebook/&quot;&gt;scale Mercurial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;POSH is a novel system for parallelizing IO-intensive shell scripts by performing work “close to the data”. The paper is one component of an exciting thread of research that could lead to significant improvements to user experience - given that technical folks from many different backgrounds use the shell every day, these improvements would be high impact.&lt;/p&gt;

&lt;p&gt;Next week I will move on from this series and into papers from Usenix ATC and OSDI. As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read. Until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This is the fourth paper in a series on “The Future of the Shell”Here are links to Part 1, Part 2, and Part 3. . These weekly paper reviews can be delivered weekly to your inbox, or you can subscribe to the new Atom feed. Over the next few weeks I will be reading papers from Usenix ATC and OSDI - as always, feel free to reach out on Twitter with feedback or suggestions about papers to read!</summary></entry><entry><title type="html">PaSh: Light-touch Data-Parallel Shell Processing</title><link href="http://www.micahlerner.com/2021/07/31/pash-light-touch-data-parallel-shell-processing.html" rel="alternate" type="text/html" title="PaSh: Light-touch Data-Parallel Shell Processing" /><published>2021-07-31T00:00:00-07:00</published><updated>2021-07-31T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/07/31/pash-light-touch-data-parallel-shell-processing</id><content type="html" xml:base="http://www.micahlerner.com/2021/07/31/pash-light-touch-data-parallel-shell-processing.html">&lt;p&gt;&lt;em&gt;This week’s paper review is the third in a series on “The Future of the Shell”&lt;label for=&quot;series&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;series&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Here are links to &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;Part 1&lt;/a&gt; and &lt;a href=&quot;/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html&quot;&gt;Part 2&lt;/a&gt; &lt;/span&gt;. These weekly paper reviews can &lt;a href=&quot;https://tinyletter.com/micahlerner/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;, and based on feedback last week I added an &lt;a href=&quot;https://www.micahlerner.com/feed.xml&quot;&gt;Atom feed&lt;/a&gt; to the site. Over the next few weeks I will be reading papers from &lt;a href=&quot;https://www.usenix.org/conference/atc21&quot;&gt;Usenix ATC&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/osdi21&quot;&gt;OSDI&lt;/a&gt; - as always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1145/3447786.3456228&quot;&gt;PaSh: Light-touch Data-Parallel Shell Processing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper discusses &lt;em&gt;PaSh&lt;/em&gt;, a system designed to automatically parallelize shell scripts. It accomplishes this goal by transforming a script into a graph of computation, then reworking the graph to enhance parallelism. To ensure that this transformation process is correct, PaSh analyzes a command as annotated with a unique configuration language - this configuration explicitly defines a command’s inputs, outputs, and parallelizability. The final step in applying PaSh transforms the intermediate graph of computation back into a shell script, although the rewritten version of the shell script will (hopefully) be able to take advantage of greater parallelism.&lt;/p&gt;

&lt;p&gt;On its own, PaSh can provide dramatic speedups to shell scripts (applying the system to common Unix one-liners accelerates them by up to 60x!), and it would certainly be interesting to see it coupled with other recent innovations in the shell (as discussed in other paper reviews in this series).&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper makes three main contributions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A study of shell commands and their parallelizability. The outcome of this study are categorizations of shell commands based on their behavior when attempting to parallelize them, and an annotation language used to describe this behavior.&lt;/li&gt;
  &lt;li&gt;A dataflow model&lt;label for=&quot;dataflow&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dataflow&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;There are many interesting papers that implement a dataflow model, although a foundational one is &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2007/03/eurosys07.pdf&quot;&gt;Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks&lt;/a&gt; &lt;/span&gt; useful for representing a shell script. A given script’s model is informed by the commands in it and the behavior of those commands (each command’s behavior is described with the custom annotation language mentioned above). The intermediate dataflow model is reworked to enhance parallelism, and is finally transformed back into a shell script (which then executes a more parallel version of the original computation).&lt;/li&gt;
  &lt;li&gt;A runtime capable of executing the output shell script, including potentially custom steps that the PaSh system introduces into the script to facilitate parallelization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shell-commands-and-their-parallelizability&quot;&gt;Shell commands and their parallelizability&lt;/h2&gt;

&lt;p&gt;PaSh seeks to parallelize shell commands, so the author’s first focus on enumerating the behaviors of shell commands through this lens. The result is four categories of commands:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Stateless&lt;/em&gt; commands, as the name suggests, don’t maintain state and are analagous to a “map” function. An example of stateless commands are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tr&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;basename&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Parallelizable pure&lt;/em&gt; commands produce the “same outputs for same inputs — but maintain internal state across their entire pass”. An example is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort&lt;/code&gt; (which should produce the same sorted output for the same input, but needs to keep track of all elements) or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt; (which should produce the same count for a given input, but needs to maintain a counter).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Non-parallelizable pure&lt;/em&gt; commands are purely functional (as above, same outputs for same inputs), but “cannot be parallelized within a single data stream”. An example command is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sha1sum&lt;/code&gt;, which hashes input data - if the stream of data passed to the command is evaluated in a different order, a different output will be generated. This is contrast to a &lt;em&gt;parallelizable pure&lt;/em&gt; command like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort&lt;/code&gt; - no matter which order you pass in unsorted data, the same output will be produced, making the computation it performs parallelizable.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Side-effectful&lt;/em&gt; commands alter the state of the system, “for example, updating environment variables, interacting with the filesystem, and accessing the network.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This categorization system is applied to commands in POSIX and GNU Coreutils.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/parclasses.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;While the vast majority of commands (in the &lt;em&gt;non-parallizable pure&lt;/em&gt; and &lt;em&gt;side-effectful&lt;/em&gt; categories) can not be parallelized without significant complication, a significant portion of commands can be relatively-easily parallelized (&lt;em&gt;stateless&lt;/em&gt; or &lt;em&gt;parallelizable pure&lt;/em&gt; categories).&lt;/p&gt;

&lt;h2 id=&quot;shell-command-annotation&quot;&gt;Shell command annotation&lt;/h2&gt;

&lt;p&gt;To describe the &lt;em&gt;parallelizability class&lt;/em&gt; of a given command and argument, the paper’s authors built an &lt;em&gt;extensibility framework&lt;/em&gt;. This framework contains two components: an &lt;em&gt;annotation language&lt;/em&gt; and &lt;em&gt;custom aggregators&lt;/em&gt; which collect the output of commands executing in parallel and stream output to the next command in the script.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;annotation language&lt;/em&gt;&lt;label for=&quot;textproto&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;textproto&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The annotation language is defined in JSON which seems helpful for initial development - it would be interesting to see the language ported to a more constrained format, like protobuf/textproto. &lt;/span&gt; is used to produce &lt;em&gt;annotations&lt;/em&gt; that indicate inputs, outputs, and parallelization class on a per-command basis&lt;label for=&quot;opensource&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;opensource&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The entirety of PaSh, including annotations, are &lt;a href=&quot;https://github.com/binpash/pash&quot;&gt;open source on GitHub&lt;/a&gt; - the docs include a &lt;a href=&quot;https://github.com/binpash/pash/blob/main/annotations/README.md#how-to-annotate-a-command&quot;&gt;useful reference for annotating commands&lt;/a&gt;. &lt;/span&gt;. An example for the default behavior of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; command is below:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;command&quot;: &quot;cut&quot;, { &quot;predicate&quot;: &quot;default&quot;, &quot;class&quot;: &quot;stateless&quot;, &quot;inputs&quot;: [&quot;stdin&quot;], &quot;outputs&quot;: [&quot;stdout&quot;] }&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;One can also define behavior for specific command and argument combinations. For example, providing (or omitting) a specific argument might change the parallelizability class or inputs/outputs for a command - an example of this is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-z&lt;/code&gt; operand (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; reads from stdin if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-z&lt;/code&gt; operand is not provided):&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ &quot;predicate&quot;: {&quot;operator&quot;: &quot;exists&quot;, &quot;operands&quot;: [ &quot;-z&quot; ]}, &quot;class&quot;: &quot;n-pure&quot;, &quot;inputs&quot;: [ &quot;args[:]&quot; ], &quot;outputs&quot;: [ &quot;stdout&quot; ] }&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The second component of the extensibility framework are &lt;em&gt;custom aggregators&lt;/em&gt; that coalesce the results of parallelized operations into a result stream - an example aggregator takes in two streams of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt; results and produces an element in an output stream. The &lt;a href=&quot;https://github.com/binpash/pash/tree/main/runtime/agg/py&quot;&gt;PaSh open source project&lt;/a&gt; includes a more complete set of examples that can aggregate the results of other parallelized commands, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uniq&lt;/code&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/agg.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;Example aggregator that takes two input streams and reduces them to an output&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The implementation of PaSh relies on the annotation language and custom aggregators described above in order to transform a shell script into an intermediate state where the script’s commands are represented with a &lt;em&gt;dataflow model&lt;/em&gt; - this transformation is covered in the next section.&lt;/p&gt;

&lt;h2 id=&quot;transforming-a-script&quot;&gt;Transforming a script&lt;/h2&gt;

&lt;p&gt;There are multiple steps involved in transforming a script to a more-parallel form. First, PaSh parses it, then transforms the parsed script into an intermediate state called a &lt;em&gt;dataflow graph&lt;/em&gt; (DFG) model. The annotation language facilitates this transformation by providing hints to the system about each step’s inputs, outputs, and behavior when parallelized. The intermediate dataflow model can be reconfigured to produce a more parallel of the script’s associated commands. Lastly, the resulting graph is transformed back into a shell script.&lt;/p&gt;

&lt;p&gt;PaSh implements this transformation process end-to-end using three components: a &lt;em&gt;frontend&lt;/em&gt;, the &lt;em&gt;dataflow model&lt;/em&gt;, and the &lt;em&gt;backend&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;frontend&lt;/em&gt; first parses the provided script into an Abstract Syntax Tree (AST). From there, it “performs a depth-first search” on the AST of the shell script, building &lt;em&gt;dataflow regions&lt;/em&gt; as it goes along - a &lt;em&gt;dataflow region&lt;/em&gt; corresponds to a section of the script that can be parallelized without hitting a “barrier” that enforces sequential execution&lt;label for=&quot;careful&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;careful&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;It’s worth noting that PaSh also is very conservative when it comes to building &lt;em&gt;dataflow regions&lt;/em&gt; that it might try to parallelize later - if an annotation for a command is not defined or it is possible that the parallelizability class of an expression could be altered (for example, if the command relies on an environment variable), then PaSh will not parallelize it. &lt;/span&gt;. Examples of barriers are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;&amp;amp;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;||&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;;&lt;/code&gt;.  The resulting &lt;em&gt;dataflow model&lt;/em&gt; is essentially a graph where the nodes are commands and edges indicate command inputs or outputs.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/dfg.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Once the dataflow graphs (DFG) for a script are produced, they can be reworked to take advantage of user-configured parallelism called &lt;em&gt;width&lt;/em&gt;. The paper lays out the formal basis for this transformation - as an example, stateless and parallelizable pure commands can be reconfigured to enhance parallelism while still producing the same result (the paper presents the idea that for these two parallelizability classes, the result of reordering nodes is the same).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/pash/stateless.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The paper also mentions other useful transformations of the graph - one in particular, adding a relay node between two nodes, is useful for enhancing performance (as described in the next section).&lt;/p&gt;

&lt;p&gt;After all transformations are performed on the graph, the &lt;em&gt;Backend&lt;/em&gt; transforms the graph back into an executable shell script.&lt;/p&gt;

&lt;h2 id=&quot;runtime&quot;&gt;Runtime&lt;/h2&gt;

&lt;p&gt;The output script generated by the application of PaSh can be difficult to execute for several reasons outlined by the paper, although this paper review doesn’t include a complete description of the runtime challenges&lt;label for=&quot;runtime&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;runtime&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper does a better job of noting them than I could and I highly recommend digging in! &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;One particularly important detail of the runtime is an approach to overcoming the shell’s “unusual laziness” - the paper notes that the “shell’s evaluation strategy is unusually lazy, in that most commands and shell constructs consume their inputs only when they are ready to process more.” To ensure high resource utilization, PaSh inserts “relay nodes”, which “consume input eagerly while attempting to push data to the output stream, forcing upstream nodes to produce output when possible while also preserving task-based parallelism”. For a number of reasons, other approaches to solving the “eagerness” problem (like not addressing it or using files) result in less performant or even possibly incorrect implementations - this comes into play in the evaluation section of the paper.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;The evaluation section of the PaSh paper includes a number of applications, but I choose to focus on three applications that stuck out to me: Common Unix One-liners, NOAA weather analysis, and Wikipedia web indexing.&lt;/p&gt;

&lt;p&gt;Applying PaSh to the set of common UNIX one-liners exercises a variety of different scripts that use &lt;em&gt;stateless&lt;/em&gt;, &lt;em&gt;parallizable pure&lt;/em&gt;, and &lt;em&gt;non-parallelizable pure&lt;/em&gt; in different configurations and numbers, speeding up scripts by up to 60x. This set of tests also demonstrates that implementation details like eager evaluating (outlined in the &lt;em&gt;Runtime&lt;/em&gt; section above) make a difference&lt;label for=&quot;benchmark&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;benchmark&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;This result is shown by benchmarking against versions of PaSh without the implementation or with a different, blocking implementation. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The next example I chose applies PaSh to an example data pipeline that analyzes NOAA weather data. PaSh is applied to the entire pipeline and achieves significant speedups - this example is particularly useful at demonstrating that the system can help to parallelize non-compute bound pipelines (the NOAA example downloads a signficant amount of data over the network). In particular, downloading large amounts of data over the network seems to closely relate to ideas discussed in the &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;first paper in this series&lt;/a&gt;, which mentions avoding redundant computation - parallelizing network requests automatically while ensuring that none are repeated unnecessarily (if the script was rerun or slightly changed) would be amazing!&lt;/p&gt;

&lt;p&gt;The last example I choose to include in this evaluation section is of Wikipedia web indexing - PaSh is able to achieve a 12X speedup when extracting text from a large body of Wikipedia’s HTML. This example uses scripts written in Python and Javascript, showcasing PaSh’s ability to speedup a pipeline utilizing commands from many different langauges and why the shell is still such a useful tool.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PaSh presents an intriguing system for automatically transforming shell scripts into more-parallel versions of themselves. I was particularly interested in how PaSh accomplishes its goals by leveraging annotations of shell commands and arguments - it would be interesting to see an open source community sprout up around maintaining or generating these annotations&lt;label for=&quot;homebrew&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;homebrew&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;I felt some similarity to Mac’s &lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt;, where users define recipes for downloading different open source projects. &lt;/span&gt;. PaSh’s use of a dataflow-based architecture also demonstrates how powerful the paradigm is. Last but not least, I’m looking forward to seeing how a system like PaSh could fit in with other related innovations in the shell (like next week’s paper on POSH)!&lt;/p&gt;

&lt;p&gt;As always, if you have feedback feel free to reach on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;. Until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This week’s paper review is the third in a series on “The Future of the Shell”Here are links to Part 1 and Part 2 . These weekly paper reviews can be delivered weekly to your inbox, and based on feedback last week I added an Atom feed to the site. Over the next few weeks I will be reading papers from Usenix ATC and OSDI - as always, feel free to reach out on Twitter with feedback or suggestions about papers to read!</summary></entry><entry><title type="html">From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers</title><link href="http://www.micahlerner.com/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html" rel="alternate" type="text/html" title="From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers" /><published>2021-07-24T00:00:00-07:00</published><updated>2021-07-24T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers</id><content type="html" xml:base="http://www.micahlerner.com/2021/07/24/from-laptop-to-lambda-outsourcing-everyday-jobs-to-thousands-of-transient-functional-containers.html">&lt;p&gt;&lt;em&gt;This week’s paper review is the second in a series on “The Future of the Shell” (Part 1, a paper about possible ways to innovate in the shell is &lt;a href=&quot;/2021/07/14/unix-shell-programming-the-next-50-years.html&quot;&gt;here&lt;/a&gt;). As always, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; with feedback or suggestions about papers to read! These weekly paper reviews can also &lt;a href=&quot;https://tinyletter.com/micahlerner/&quot;&gt;be delivered weekly to your inbox&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/atc19-fouladi.pdf&quot;&gt;From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper discusses &lt;em&gt;gg&lt;/em&gt;, a system designed to  parallelize commands initiated from a developer desktop using cloud functions&lt;label for=&quot;firecracker&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;firecracker&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Like those running on AWS Lambda in &lt;a href=&quot;/2021/06/17/firecracker-lightweight-virtualization-for-serverless-applications.html&quot;&gt;Firecracker VMs, as discussed in a previous paper review&lt;/a&gt;. &lt;/span&gt; - an alternative summary is that &lt;em&gt;gg&lt;/em&gt; allows a developer to, for a limited time period, “rent a supercomputer in the cloud”.&lt;/p&gt;

&lt;p&gt;While parallelizing computation using cloud functions is not a new idea on its own&lt;label for=&quot;simdiff&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;simdiff&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Related systems, like &lt;a href=&quot;/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html&quot;&gt;Ray&lt;/a&gt;, are discussed later in this paper review. &lt;/span&gt;, &lt;em&gt;gg&lt;/em&gt; focuses specifically on leveraging affordable cloud compute functions to speed up applications not natively designed for the cloud, like &lt;a href=&quot;https://www.gnu.org/software/make/&quot;&gt;make&lt;/a&gt;-based build systems (common in open source projects), unit tests, and video processing pipelines.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper’s contains two primary contributions: the design and implementation of &lt;em&gt;gg&lt;/em&gt; (a general system for parallelizing command line operations using a computation graph executed with cloud functions) and the application of &lt;em&gt;gg&lt;/em&gt; to several domains (including unit testing, software compilation, and object recognition).&lt;/p&gt;

&lt;p&gt;To accomplish the goals of &lt;em&gt;gg&lt;/em&gt;, the authors needed to overcome three challenges: managing software dependencies for the applications running in the cloud, limiting round trips from the developer’s workstation to the cloud (which can be incurred if the developer’s workstation coordinates cloud executions), and making use of cloud functions themselves.&lt;/p&gt;

&lt;p&gt;To understand the paper’s solutions to these problems, it is helpful to have context on several areas of related work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Process migration and outsourcing&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; aims to outsource computation from the developer’s workstation to remote nodes. Existing systems like &lt;a href=&quot;https://distcc.github.io/&quot;&gt;distcc&lt;/a&gt; and &lt;a href=&quot;https://github.com/icecc/icecream&quot;&gt;icecc&lt;/a&gt; use remote resources to speed up builds, but often require long-lived compute resources, potentially making them more expensive to use. In contrast, &lt;em&gt;gg&lt;/em&gt; uses cloud computing functions that can be paid for at the second or millisecond level.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Container orchestration systems&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; runs computation in cloud functions (effectively containers in the cloud). Existing container systems, like Kubernetes or Docker Swarm, focus on the actual scheduling and execution of tasks, but don’t necessarily concern themselves with executing dynamic computation graphs - for example, if Task B’s inputs are the output of Task A, how can we make the execution of Task A fault tolerant and/or memoized.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Workflow systems&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; transforms an application into small steps that can be executed in parallel. Existing systems following a similar model (like Spark) need to be be programmed for specific tasks, and are not designed for “everyday” applications that a user would spawn from the command line. While Spark can call system binaries, the binary is generally installed on all nodes, where each node is long-lived. In contast, &lt;em&gt;gg&lt;/em&gt; strives to provide the minimal dependencies and data required by a specific step - the goal of limiting dependencies also translates into lower overhead for computation, as less data needs to be transferred before a step can execute. Lastly, systems like Spark are accessed through language bindings, whereas &lt;em&gt;gg&lt;/em&gt; aims to be language agnostic.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Burst-parallel cloud functions&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; aims to be a higher-level and more general system for running short-lived cloud functions than existing approaches - the paper cites &lt;a href=&quot;http://pywren.io/&quot;&gt;PyWren&lt;/a&gt; and &lt;a href=&quot;https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/fouladi&quot;&gt;ExCamera&lt;/a&gt; as two systems that implement specific functions using cloud components (a MapReduce-like framework and video encoding, respectively). In contrast, &lt;em&gt;gg&lt;/em&gt; aims to provide, “common services for dependency management, straggler mitigation, and scheduling.”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Build tools&lt;/em&gt;: &lt;em&gt;gg&lt;/em&gt; aims to speed up multiple types of applications through parallelization in the cloud. One of those applications, compiling software, is addressed by systems like &lt;a href=&quot;https://bazel.build/&quot;&gt;Bazel&lt;/a&gt;, &lt;a href=&quot;https://www.pantsbuild.org/&quot;&gt;Pants&lt;/a&gt;, and &lt;a href=&quot;https://buck.build&quot;&gt;Buck&lt;/a&gt;. These newer tools are helpful for speeding up builds by parallelizing and incrementalizing operations, but developers will likely not be able to use advanced features of the aforementioned systems unless they rework their existing build.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we understand more about the goals of &lt;em&gt;gg&lt;/em&gt;, let’s jump into the system’s design and implementation.&lt;/p&gt;

&lt;h2 id=&quot;design-and-implementation-of-gg&quot;&gt;Design and implementation of gg&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;gg&lt;/em&gt; comprises three main components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;em&gt;gg Intermediate Representation (gg IR)&lt;/em&gt; used to represent the units of computation involved in an application - &lt;em&gt;gg IR&lt;/em&gt; looks like a graph, where dependencies between steps are the edges and the units of computation/data are the nodes.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Frontends&lt;/em&gt;, which take an application and generate the intermediate representation of the program.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Backends&lt;/em&gt;, which execute the &lt;em&gt;gg IR&lt;/em&gt;, store results, and coalesce them when producing output.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;em&gt;gg Intermediate Representation (gg IR)&lt;/em&gt; describes the steps involved in a given execution of an application&lt;label for=&quot;dynamic&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dynamic&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Notably, this graph is dynamic and lazily evaluated, which is helpful for supporting applications that involve “loops, recursion, or other non-DAG dataflows. &lt;/span&gt;. Each step is described as a &lt;em&gt;thunk&lt;/em&gt;, and includes the command that the step invokes, environment variables, the arguments to that command, and all inputs. Thunks can also be used to represent primitive values that don’t need to be evaluated - for example, binary files like gcc need to be used in the execution of a thunk, but do not need to be executed. A &lt;em&gt;thunk&lt;/em&gt; is identified using a content-addressing scheme&lt;label for=&quot;content&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;content&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper describes a content-addressing scheme where, “the name of an object has four components: (1) whether the object is a primitive value (hash starting with V) or represents the result of forcing some other thunk (hash starting with T), (2) a SHA-256 hash, (3) the length in bytes, and (4) an optional tag that names an object or a thunk’s output.” &lt;/span&gt; that allows one &lt;em&gt;thunk&lt;/em&gt; to depend on another (by specifying the objects array as described in the figure below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/thunks.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Frontends&lt;/em&gt; produce the &lt;em&gt;gg IR&lt;/em&gt;, either through a language-specific SDK (where a developer describes an application’s execution in code)&lt;label for=&quot;ray&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ray&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;This seems like it would have a close connection to &lt;a href=&quot;/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html&quot;&gt;Ray, another previous paper review&lt;/a&gt;. &lt;/span&gt; or with a &lt;em&gt;model substitution primitive&lt;/em&gt;. The model substitution primitive mode uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gg infer&lt;/code&gt; to generate all of the thunks (a.k.a. steps) that would be involved in the execution of the original command. This command executes based on advanced knowledge of how to model specific types of systems - as an example, imagine defining a way to process projects that use &lt;em&gt;make&lt;/em&gt;. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gg infer&lt;/code&gt; is capable of converting the aforementioned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make&lt;/code&gt; command into a set of thunks that will compile independent C++ files in parallel, coalescing the results to produce the intended binary - see the figure below for a visual representation.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/ggir.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Backends&lt;/em&gt; execute the &lt;em&gt;gg IR&lt;/em&gt; produced by the &lt;em&gt;Frontends&lt;/em&gt; by “forcing” the execution of the thunk that corresponds to the output of the application’s execution. The computation graph is then traced backwards along the edges that lead to the final output. Backends can be implemented on different cloud providers, or even use the developer’s local machine. While the internals of the backends may differ, each backend must have three high-level components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Storage engine&lt;/em&gt;: used to perform CRUD operations for content-addressable outputs (for example, storing the result of a thunk’s execution).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Execution engine&lt;/em&gt;: a function that actually performs the execution of a thunk, abstracting away actual execution. It must support, “a simple abstraction: a function that receives a thunk as the input and returns the hashes of its output objects (which can be either values or thunks)”. Examples of execution engines are “a local multicore machine, a cluster of remote VMs, AWS Lambda, Google Cloud Functions, and IBM Cloud Functions (OpenWhisk)”.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Coordinator&lt;/em&gt;: The coordinator is a process that orchestrates the execution of a &lt;em&gt;gg IR&lt;/em&gt; by communicating with one or more execution engines and the storage engine&lt;label for=&quot;coordinator&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;coordinator&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;It was unclear from the paper whether multiple storage engines can be associated with a single coordinator. &lt;/span&gt;. It provides higher level services like making scheduling decisions, memoizing thunk execution (not rerunning a thunk unnecessarily), rerunning thunks if they fail, and straggler mitigation&lt;label for=&quot;straggler&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;straggler&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Straggler mitigation in this context means ensuring that slow-running &lt;em&gt;thunks&lt;/em&gt; do not impact overall execution time. One strategy to address this issue is uunning multiple copies of a thunk in parallel, then continuing after the first succeds - likely possible because content-addressable nature of thunks means that their execution is idempotent. &lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;applying-and-evaluating-gg&quot;&gt;Applying and evaluating gg&lt;/h2&gt;

&lt;p&gt;The &lt;em&gt;gg&lt;/em&gt; system was applied to, and evaluated against, four&lt;label for=&quot;five&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;five&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper also includes an implementation of recursive fibonacci to demonstrate that &lt;em&gt;gg&lt;/em&gt; can handle dynamic execution graphs while also memoizing redundant executions. &lt;/span&gt; use cases: software compilation, unit testing, video encoding, and object recognition.&lt;/p&gt;

&lt;p&gt;For software compilation, FFmpeg, GIMP, Inkscape, and Chromium were compiled either locally, using a distributed build tool (icecc), or with &lt;em&gt;gg&lt;/em&gt;. For medium-to-large programs, (Inkscape and Chromium), &lt;em&gt;gg&lt;/em&gt; performed better than the alternatives with an &lt;em&gt;AWS Lambda&lt;/em&gt; execution engine, likely because it is better able to handle high degrees of parallelism - a &lt;em&gt;gg&lt;/em&gt; based compilation is able to perform all steps remotely, whereas the two other systems perform bottlenecking-steps at the root node. The paper also includes an interesting graphic outlining the behavior of &lt;em&gt;gg&lt;/em&gt; worker’s during compilation, which contains an interesting visual of straggler mitigation (see below).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/gg/stragglers.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;For unit testing, the LibVPX test suite was built in parallel with &lt;em&gt;gg&lt;/em&gt; on AWS Lambda, and compared with a build box - the time differences between the two strategies was small, but that authors argue that the &lt;em&gt;gg&lt;/em&gt; based solution was able to provide results earlier because of its parallelism.&lt;/p&gt;

&lt;p&gt;For video encoding, &lt;em&gt;gg&lt;/em&gt; performed worse than an optimized implementation (based on ExCamera), although the &lt;em&gt;gg&lt;/em&gt; based system introduces memoization and fault tolerance.&lt;/p&gt;

&lt;p&gt;For object recognition, &lt;em&gt;gg&lt;/em&gt; was compared to &lt;a href=&quot;https://scanner.run&quot;&gt;Scanner&lt;/a&gt;&lt;label for=&quot;scanner&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;scanner&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;“Scanner is a distributed system for building efficient video processing applications that scale.” - it would be interesting to see this implemented in Ray! &lt;/span&gt;, and observed significant speedups&lt;label for=&quot;speedup&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;speedup&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The authors mention that the &lt;em&gt;gg&lt;/em&gt; implementation was specifically tuned to the task. &lt;/span&gt; that the authors attribute to &lt;em&gt;gg&lt;/em&gt;’s scheduling algorithm and removing abstraction in Scanner’s design.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While &lt;em&gt;gg&lt;/em&gt; seems like an exciting system for scaling command line applications, it may not be the best fit for every project (as indicated by the experimental results) - in particular, &lt;em&gt;gg&lt;/em&gt; seems well positioned to speed up traditional make-based builds without requiring a large-scale migration. The paper authors also note limitations of the system, like &lt;em&gt;gg&lt;/em&gt;’s incompatibility with GPU programs - &lt;a href=&quot;/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html&quot;&gt;my previous paper review on Ray&lt;/a&gt; seems relevant to adapting &lt;em&gt;gg&lt;/em&gt; in the future.&lt;/p&gt;

&lt;p&gt;A quote that I particularly enjoyed from the paper’s conclusion was this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As a computing substrate, we suspect cloud functions are in a similar position to Graphics Processing Units in the 2000s. At the time, GPUs were designed solely for 3D graphics, but the community gradually recognized that they had become programmable enough to execute some parallel algorithms unrelated to graphics. Over time, this “general-purpose GPU” (GPGPU) movement created systems-support technologies and became a major use of GPUs, especially for physical simulations and deep neural networks. Cloud functions may tell a similar story. Although intended for asynchronous microservices, we believe that with sufficient effort by this community the same infrastructure is capable of broad and exciting new applications. Just as GPGPU computing did a decade ago, nontraditional “serverless” computing may have far-reaching effects.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thanks for reading, and feel free to reach out with feedback on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; - until next time!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This week’s paper review is the second in a series on “The Future of the Shell” (Part 1, a paper about possible ways to innovate in the shell is here). As always, feel free to reach out on Twitter with feedback or suggestions about papers to read! These weekly paper reviews can also be delivered weekly to your inbox.</summary></entry><entry><title type="html">Unix Shell Programming: The Next 50 Years (The Future of the Shell, Part I)</title><link href="http://www.micahlerner.com/2021/07/14/unix-shell-programming-the-next-50-years.html" rel="alternate" type="text/html" title="Unix Shell Programming: The Next 50 Years (The Future of the Shell, Part I)" /><published>2021-07-14T00:00:00-07:00</published><updated>2021-07-14T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/07/14/unix-shell-programming-the-next-50-years</id><content type="html" xml:base="http://www.micahlerner.com/2021/07/14/unix-shell-programming-the-next-50-years.html">&lt;p&gt;&lt;a href=&quot;https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s06-greenberg.pdf&quot;&gt;Unix Shell Programming: The Next 50 Years&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week’s paper won the distinguished presentation award at HotOS 2021, and discusses the potential for future innovation in the tool that many use every day - the shell!&lt;label for=&quot;hn&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;hn&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;A previous submission of this paper on &lt;a href=&quot;https://news.ycombinator.com/item?id=27378444&quot;&gt;Hacker News&lt;/a&gt; elicited a number of strong reactions. One reaction was the assertion that there are in fact modern shells - &lt;a href=&quot;https://elv.sh/&quot;&gt;Elvish&lt;/a&gt; features most prominently. While I think several of the comments on the original posting are in the right direction, my takeaway from this paper was that modern shells could take advantage of exciting advances in other areas of systems research (in particular, data flow and transparent parallelization of computation). &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It not only proposes a way forward to address (what the authors view) as the shell’s sharp edges, but also references a number of other interesting papers that I will publish paper reviews of over the next few weeks - the gist of several of the papers are mentioned further on in this article:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3447786.3456228&quot;&gt;PaSh: light-touch data-parallel shell processing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/atc20/presentation/raghavan&quot;&gt;POSH: A Data-Aware Shell&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/atc19/presentation/fouladi&quot;&gt;From Laptop to Lambda:  Outsourcing Everyday Jobs to Thousands  of Transient Functional Containers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-good-the-bad-and-the-ugly&quot;&gt;The good, the bad, and the ugly&lt;/h2&gt;

&lt;p&gt;In &lt;em&gt;Unix Shell Programming: The Next 50 Years&lt;/em&gt;, the authors argue that while the shell is a powerful tool, it can be improved for modern users and workflows. To make this argument, the paper first considers “the good, the bad, and the ugly” of shells in order to outline what should (or should not) change in shells going forward.&lt;/p&gt;

&lt;p&gt;The paper identifies four &lt;em&gt;good&lt;/em&gt; components of modern shells:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Universal composition&lt;/em&gt;: The shell already prioritizes chaining small programs working in concert (which can be written in many different languages), according to the Unix philosophy.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Stream processing&lt;/em&gt;: The shell is well structured to perform computation that flows from one command to another through pipes (for example, using xargs). The paradigm of stream processing is an active area of research outside of the shell and shows up in modern modern distributed systems like &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Apache Flink&lt;/a&gt; or &lt;a href=&quot;https://spark.apache.org/streaming/&quot;&gt;Spark Streaming&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Unix-native&lt;/em&gt;: “The features and abstractions of the shell are well suited to the Unix file system and file-based abstractions. Unix can be viewed as a naming service, mapping strings to longer strings, be it data files or programs”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Interactive&lt;/em&gt;: A REPL-like environment for interacting with your system translates into user efficiency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next - four &lt;em&gt;bad&lt;/em&gt; features are detailed, with the note that, “It’s hard to imagine ‘addressing’ these characteristics without turning the shell into something it isn’t; it’s hard to get the good of the shell without these bad qualities”&lt;label for=&quot;wordexp&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;wordexp&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;As an example, the paper links to &lt;a href=&quot;https://cs.pomona.edu/~michael/papers/px2018.pdf&quot;&gt;previous research&lt;/a&gt; that word expansion (“the conversion of user input into…a command and its arguments”) make up a significant portion of user commands. &lt;/span&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Too arbitrary&lt;/em&gt;: Almost any command can be executed as part of a shell pipeline&lt;label for=&quot;shelltetris&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;shelltetris&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://www.unix.com/shell-programming-and-scripting/174525-tetris-game-based-shell-script-new-algorithm.html&quot;&gt;Shell tetris&lt;/a&gt;! &lt;/span&gt;. While this flexibility is useful for interacting with many different components (each of which may be in a different language), the arbitrariness of a shell makes formalizing a shell’s behavior significantly more difficult.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Too dynamic&lt;/em&gt;: Shell behavior can depend on runtime execution state, making analysis of shell scripts more difficult (analysis techniques could be helpful for determining undesirable outcomes of shell scripts before running them).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Too obscure&lt;/em&gt;: There is a 300 page specification for the &lt;a href=&quot;https://pubs.opengroup.org/onlinepubs/9699919799.2018edition/utilities/V3_chap02.html#tag_18_12&quot;&gt;POSIX shell&lt;/a&gt;, in addition to &lt;a href=&quot;http://get.posixcertified.ieee.org/testsuites.html&quot;&gt;test suites&lt;/a&gt;. Unfortunately, the authors found multiple issues with common shells, and even with the test suites themselves! The undefined nature of what a shell is actually supposed to do in specific situations means that it is hard to make guarantees about correctness&lt;label for=&quot;smoosh&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;smoosh&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://mgree.github.io/papers/popl2020_smoosh.pdf&quot;&gt;One of the author’s papers&lt;/a&gt; goes more in-depth on the question of ‘What is the POSIX shell?’ &lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, four &lt;em&gt;ugly&lt;/em&gt; components are detailed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Error proneness&lt;/em&gt;: There aren’t checks to prevent a user from making mistakes (which could have drastic conditions). &lt;a href=&quot;https://www-uxsup.csx.cam.ac.uk/misc/horror.txt&quot;&gt;Unix/Linux Horror Stories&lt;/a&gt; has some good ones (or bad, if you were the person making the mistake!).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Performance doesn’t scale&lt;/em&gt;: the shell isn’t set up to parallelize trivially parallelize problems across many cores or machines (which would be very helpful in a modern environment)&lt;label for=&quot;gg&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;gg&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;If this is interesting to you, predominantly all of the papers in the series discuss with this problem. &lt;/span&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Redundant recomputation&lt;/em&gt;: If a developer makes a change to a shell script, they will have to rerun it in its entirety (unless they are a shell wizard and have gone out of their way to ensure that their script does not do so, while potentially making operations idempotent).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;No support for contemporary deployments&lt;/em&gt;: Similar to the 2nd point - most shell scripts aren’t designed to take advantage of multiple machines, nor of cloud deployments.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;enabling-the-shell-to-move-forward&quot;&gt;Enabling the shell to move forward&lt;/h2&gt;

&lt;p&gt;The paper next argues that two sets of recent academic research are enabling the shell to move forward: &lt;em&gt;formalizing the shell&lt;/em&gt; and &lt;em&gt;annotation languages&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Recent work on &lt;em&gt;formalizing the shell&lt;/em&gt; is detailed in &lt;a href=&quot;https://mgree.github.io/papers/popl2020_smoosh.pdf&quot;&gt;Executable Formal Semantics for the POSIX Shell&lt;/a&gt;, which has two major components: &lt;em&gt;Smoosh&lt;/em&gt; and &lt;em&gt;libdash&lt;/em&gt; - the artifacts for &lt;a href=&quot;https://github.com/mgree/smoosh&quot;&gt;both are open source&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Smoosh&lt;/em&gt; is an executable shell specification written in &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2692915.2628143&quot;&gt;Lem&lt;/a&gt;&lt;label for=&quot;Smoosh&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;Smoosh&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Which can then be translated to different formats, including proof languages like Coq &lt;/span&gt;. A shell specification written in code (versus the extensive written specification) meant that the aforementioned paper was able to test various shells for undefined behavior, in the process finding several bugs in implementation (not to mention, bugs in the test suite for the POSIX shell specification!)&lt;label for=&quot;smoosh&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;smoosh&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Another interesting feature of &lt;em&gt;Smoosh&lt;/em&gt; is that it provides two interfaces to interact with the OS - one actually invokes syscalls, whereas the other mode simulates syscalls (and is used for &lt;a href=&quot;https://www.cs.umd.edu/~mwh/se-tutorial/symbolic-exec.pdf&quot;&gt;symbolic execution&lt;/a&gt;). This vaguely reminds me of the testing system used in &lt;a href=&quot;/2021/06/12/foundationdb-a-distributed-unbundled-transactional-key-value-store.html&quot;&gt;FoundationDB&lt;/a&gt;, covered in a previous paper review. &lt;/span&gt;.  &lt;em&gt;libdash&lt;/em&gt; transforms shell scripts from (or to) abstract syntax trees, and is used by &lt;em&gt;Smoosh&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Annotation languages&lt;/em&gt; can allow users to specify how a command runs, in addition to possible inputs and outputs. Strictly specifying a command allows for it to be included as a step (with inputs and outputs) in a data flow graph, enabling more advanced functionality - for example, deciding to divide the inputs of a step across many machines, perform computation in parallel, then coalescing the output. If this type of advanced functionality sounds interesting to you, stay tuned! I’ll be reading about the two papers that fall into this category (PaSH &amp;amp; POSH) over the next few weeks.&lt;/p&gt;

&lt;p&gt;After discussing these two research areas, the paper discusses a new project from the authors, called Jash (Just Another SHell). It can act as a shim between the user and the actual execution of a shell command. Eventually, Jash seems like it could implement functionality similar to an execution engine or query planner, evaluating commands at runtime and deciding how to perform the requested work (providing feedback to the user if the script will produce unintended side effects).&lt;/p&gt;

&lt;h2 id=&quot;the-future&quot;&gt;The future&lt;/h2&gt;

&lt;p&gt;The paper outlines five functionalities for the future of the shell:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Distribution&lt;/em&gt;: in the context of a shell, this means building a system capable of scaling beyond a single machine (for example, inserting compute resources at different stages of a shell command’s execution to parallelize) - all three of the papers in this series dive deep on this idea.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Incremental support&lt;/em&gt;: if a shell script is changed slightly, but can reuse previous computation, a shell could strive to do so.&lt;label for=&quot;dd&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper cites &lt;a href=&quot;https://github.com/TimelyDataflow/differential-dataflow&quot;&gt;Differential Dataflow&lt;/a&gt;, which is related to another paper I have had on the backlog for a while - &lt;a href=&quot;http://sigops.org/s/conferences/sosp/2013/papers/p439-murray.pdf&quot;&gt;Naiad: A Timely Dataflow System&lt;/a&gt;. &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Heuristic support&lt;/em&gt;: While transforming a shell script into a data flow graph can be facilitated by &lt;em&gt;annotation languages&lt;/em&gt;, it would be costly to annotate every shell command. Ideally, the annotation of commands could be performed automatically (or with the support of automation).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;User support&lt;/em&gt;: A shell should take advantage of modern features like language servers. A formal specification for interacting with the shell can theoretically simplify interactions with the shell.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Formal support&lt;/em&gt;: The paper cites how formalization has helped C “tool authors and standards writers”, in particular with respect to undefined behavior. Diving deep on this, I found a &lt;a href=&quot;http://people.csail.mit.edu/nickolai/papers/wang-undef.pdf&quot;&gt;few&lt;/a&gt; helpful papers that discuss undefined C behavior - in particular &lt;a href=&quot;https://blog.regehr.org/archives/1520&quot;&gt;this one&lt;/a&gt; from Pascal Cuoq and John Regehr).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The shell is an integral part of systems, and this paper makes a case for revisiting the shell’s sharp edges, while revamping its functionality for modern use cases. I’m excited to keep diving deep on this topic - this is the first post in a series I’m doing! If you enjoyed it (or otherwise have suggestions), find me on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;. Until next time.&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Unix Shell Programming: The Next 50 Years</summary></entry><entry><title type="html">Breakfast of Champions: Towards Zero-Copy Serialization with NIC Scatter-Gather</title><link href="http://www.micahlerner.com/2021/07/07/breakfast-of-champions-towards-zero-copy-serialization-with-nic-scatter-gather.html" rel="alternate" type="text/html" title="Breakfast of Champions: Towards Zero-Copy Serialization with NIC Scatter-Gather" /><published>2021-07-07T17:00:00-07:00</published><updated>2021-07-07T17:00:00-07:00</updated><id>http://www.micahlerner.com/2021/07/07/breakfast-of-champions-towards-zero-copy-serialization-with-nic-scatter-gather</id><content type="html" xml:base="http://www.micahlerner.com/2021/07/07/breakfast-of-champions-towards-zero-copy-serialization-with-nic-scatter-gather.html">&lt;p&gt;&lt;em&gt;This week’s paper is from HotOS 2021. Many of the HotOS papers (like this one) propose future directions for Operating Systems research, in addition to including prototypes (in contrast with other papers that focus on a single system’s implementation). The full proceedings of the conference are &lt;a href=&quot;https://sigops.org/s/conferences/hotos/2021/&quot;&gt;here&lt;/a&gt; - as always, if there are any papers that stick out to you, feel free to reach out on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sigops.org/s/conferences/hotos/2021/papers/hotos21-s10-raghavan.pdf&quot;&gt;Breakfast of Champions: Towards Zero-Copy Serialization with NIC Scatter-Gather&lt;/a&gt; Deepti Raghavan, Philip Levis, Matei Zaharia, Irene Zhang.&lt;/p&gt;

&lt;p&gt;This paper (written by authors from Stanford and Microsoft Research) focuses on how to speed up data serialization associated with Remote Procedure Call (RPC)&lt;label for=&quot;rpc&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;rpc&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Martin Kleppman, the author of the amazing &lt;a href=&quot;https://dataintensive.net/&quot;&gt;‘Designing Data-Intensive Applications’&lt;/a&gt; has a useful lecture on RPC &lt;a href=&quot;https://www.youtube.com/watch?v=S2osKiqQG9s&quot;&gt;here&lt;/a&gt;. &lt;/span&gt; systems in the datacenter. Normally, RPC systems are not focused on as a performance bottleneck, but the authors argue that as we &lt;a href=&quot;https://cacm.acm.org/magazines/2017/4/215032-attack-of-the-killer-microseconds/fulltext&quot;&gt;enter the “microsecond era”&lt;/a&gt;&lt;label for=&quot;microsecond&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;microsecond&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The microsecond era refers to a time when ‘low-latency I/IO devices, ranging from faster datacenter networking to emerging non-volatile memories and accelerators’ are used more widely in industry. A shorter intro is &lt;a href=&quot;https://www.youtube.com/watch?v=XEsR5kPHWeU&quot;&gt;here&lt;/a&gt; &lt;/span&gt;, the performance of previously overlooked systems will standout - CPU will become a scarce resource as data center networks become even faster.&lt;/p&gt;

&lt;p&gt;RPC systems (like &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt; and &lt;a href=&quot;https://thrift.apache.org/&quot;&gt;Apache Thrift&lt;/a&gt;) are very popular, but use CPU cycles to move memory in the process of reading data from or writing data to the network. This overhead comes from “coalescing or flattening in-memory data structures” - taking application objects that may contain pointers to separate areas of memory and moving the associated data into a contiguous area of memory (so that the combined object can be sent over the network).&lt;/p&gt;

&lt;p&gt;To limit (and possibly eliminate) this overhead, the authors suggest leveraging functions of commodity Network Interface Cards (NICs) with built in support for high performance computing primitives&lt;label for=&quot;mellanox&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;mellanox&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper includes benchmarks from using a Mellanox NIC - see &lt;a href=&quot;https://www.nextplatform.com/2015/11/12/mellanox-turns-infiniband-switches-into-mpi-accelerators/&quot;&gt;here&lt;/a&gt; for a discussion of how Mellanox can accelerate MPI operations. &lt;/span&gt;. The primitive focused on in the paper is scatter-gather&lt;label for=&quot;scattergather&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;scattergather&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://people.inf.ethz.ch/asingla/papers/socc20-scattergather.pdf&quot;&gt;Specializing the network for scatter-gather workloads&lt;/a&gt; describes a number of use cases for scatter-gather, for example: “Web services such as search engines often involve a user-facing server receiving a user request, and in turn, contact hundreds to thousands of back-end servers, e.g.,for queries across a distributed search index”. &lt;/span&gt;, which bears a strong resemblance to the function that the authors are trying to optimize (gathering disparate memory to a single location or taking a contiguous piece of memory and distributing it):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Scatter-gather was designed for high-performance computing, where applications frequently move large, statically-sized chunks of memory between servers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Even though some NICs support scatter-gather, an extra step (called &lt;em&gt;kernel bypass&lt;/em&gt;) can be taken to ensure that there are no (or at least limited) memory copies made in the serialization/deserialization process. Kernel-bypass is used to build high-speed networking stacks, for example &lt;a href=&quot;https://blog.cloudflare.com/kernel-bypass/&quot;&gt;at Cloudflare&lt;/a&gt;. The technique can make IO devices (like a NIC) available to user-space, ensuring that no unncessary memory movement in or out of the kernel occurs&lt;label for=&quot;kernelbypass&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;kernelbypass&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MpjlWt7fvrw&quot;&gt;This video&lt;/a&gt; contains an in-depth explanation of kernel-bypass for high-speed networking. One of the authors of the paper (Irene Zhang), also has an interesting paper on a system that provides abstractions for kernel-bypass: &lt;a href=&quot;https://irenezhang.net/papers/demikernel-hotos19.pdf&quot;&gt;I’m Not Dead Yet! The Role of the Operating System in a Kernel-Bypass Era&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The paper makes three main contributions: it explores the sources of RPC serialization (and deserialization) overhead, proposes a solution to limiting that overhead, and outlines potential areas of research in the future to expand on the proposed approach.&lt;/p&gt;

&lt;h2 id=&quot;the-limits-of-software-serialization&quot;&gt;The Limits of Software Serialization&lt;/h2&gt;

&lt;p&gt;To explore performance issues with software serialization (&lt;em&gt;software&lt;/em&gt; serialization because no extra hardware, like an &lt;em&gt;accelerator&lt;/em&gt;, is used), the paper includes two experimental results that highlight the reasons that CPU-based serialization is inefficient - flattening, moving, and copying data when performing RPC operations incurs CPU cycles.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/breakfast/serialize_perf.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The first experiment shows the limits of different RPC serialization/deserialization implementations for a server that is deserializing and serializing a 1024 byte string message&lt;label for=&quot;1024&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;1024&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;A simple string message was chosen because it is a can represent “the minimal overhead for serialization today.” &lt;/span&gt;(where the limit is the latency associated with a given throughput). Predominantly all of the implementations are grouped together on the performance graph, with three important gaps.&lt;/p&gt;

&lt;p&gt;First, Protobuf performs poorly relatively to the other RPC libraries (as it performs UTF-8 validation on the string message).&lt;/p&gt;

&lt;p&gt;The next gap is between an implementation using DPDK&lt;label for=&quot;dpdk&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dpdk&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://doc.dpdk.org/guides/prog_guide/overview.html&quot;&gt;DPDK&lt;/a&gt; stands for “Data Plane Development Kit”, and is a software library that allows for NICs to be programmed from user-space, facilitating high speed networking through bypassing the kernel. &lt;/span&gt; single core and the RPC libraries. The serialization libraries need to make copies when encoding to or decoding from a wire format (see graph below for a comparison between Protobuf and Cap’n Proto in this regard), and these extra copies mean lower peak throughput (because CPU cycles are spent on moving the memory). In contrast, the DPDK line indicates what would be possible performance wise if serialization libraries could manipulate shared memory with the networking stack, in turn limiting memory copies. This line sets the benchmark for what would be possible in a world where RPC libaries limited copies by integrating with a kernel-bypass library.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/breakfast/serialize.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The second experiment is included above - it shows how both Protobuf and Cap’n Proto incur copies when performing serialization/deserialization (and the time for each grow with the size of the message).&lt;/p&gt;

&lt;h2 id=&quot;leveraging-the-nic-for-serialization&quot;&gt;Leveraging the NIC for Serialization&lt;/h2&gt;

&lt;p&gt;Before introducing the scatter-gather based implementation, the paper explores the performance of splitting up a payload into differently sized chunks. This experiment uses an echo server benchmark (where a client serializes a message and sends it to a server, which returns it to the client), with the results indicating that objects below a threshold size of 256 bytes do not benefit from a zero-copy scatter-gather approach. Dividing up a payload into smaller packets can hurt performance if the packets are too small relative to the overhead of the NIC building them (where the definition of “too small” varies with the model of NIC being used, a topic that will come up in the next section&lt;label for=&quot;pcie&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;pcie&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Each scatter/gather chunk of memory is an expensive NIC-to-PCIe round-trip. Different NICs will be able to parallelize fewer or more of these calls. &lt;/span&gt;).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/breakfast/scatter_gather_array.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To use a NIC’s scatter-gather functionality to speed up serialization/deserialization, the paper implements a datastructure called a &lt;em&gt;ScatterGatherArray&lt;/em&gt; that contains a count of the number of entries, a list of pointers, and the size of each entry - “when applications call serialize, the library produces a scatter-gather array that can be passed to the networking stack instead of a single contiguous buffer”. An associated header contains a bitmap of which fields in the &lt;em&gt;ScatterGatherArray&lt;/em&gt; are filled and metadata about the type of the field. When serializing an object, the “resulting wireformat is similar to &lt;a href=&quot;https://capnproto.org/encoding.html&quot;&gt;Cap’n Proto’s wireformat&lt;/a&gt;.”&lt;/p&gt;

&lt;p&gt;The paper additionally includes details of deserializing an object. I won’t include full details on deserialization (as they are readily accessible in the paper), but there are several interesting features. For one, the paper discusses pros of a potentially different wire format where information on all fields is stored in the header (in contrast to the implementation which has a header that only includes metadata on a field if it is present) - if all fields are stored, the header would likely be larger, but deserialization could be constant time, rather than requiring a scan of all fields in the header.&lt;/p&gt;

&lt;p&gt;Most importantly, the prototype using NIC scatter-gather, kernel-bypass, the &lt;em&gt;ScatterGatherArray&lt;/em&gt;, and the wireformat almost reach DPDK single-core performance benchmark from the first experiment outlined in “The Limits of Software Serialization_ above:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The prototype implementation achieves about 9.15 Gbps (highest throughput measured under 15μs of tail latency). The prototype’s performance improves on all the serialization libraries and the 1-copy (”No Serialization”) baseline, but falls about 1.2 Gbps short of the optimal DPDK throughput.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;open-research-challenges&quot;&gt;Open Research Challenges&lt;/h2&gt;

&lt;p&gt;There are four main research challenges associated with combining NIC scatter-gather with serialization libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;NIC support for scatter gather&lt;/em&gt;: the costs of sending payloads using NIC scatter-gather aren’t worth it if the payloads are below a threshold size. Can NICs be designed with this in mind?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Using scatter-gather efficiently&lt;/em&gt;: Different application payloads may or may not work well with different NICs, as different NICs have different performance profiles with small or oddly sized payloads&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Accessing Application Memory for Zero-Copy I/O&lt;/em&gt;: Memory that is shared between the NIC and user-space must be pinned, meaning that large amounts of memory could be reserved for an application, but may never be used (or used rarely). Managing what memory is pinned, when it is pinned, and how it is allocated are all active areas of developement (with papers linked to by the authors), as a user of the approach wouldn’t want to incur enormous overhead.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Providing Zero-Copy I/O with Memory Safety&lt;/em&gt;: If the NIC and CPU are both interacting with memory concurrently, they will need a way to do so safely. Other challenges include implementing a way for deserialized responses to be reclaimed once an application is done using them (otherwise the response will stay in memory, which will eventually run out).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Kernel-bypass and the microsecond era are leading to an exciting time in systems as researchers figure out how to rework existing systems and ensure high performance. Accelerators (and abstracting away the use of them) is an area I hope to cover more in the future. While this paper is a slight deviation from the types of papers that I’ve previously reviewed on this blog, I hope you enjoyed it - if you have feedback feel free to ping me on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;. Until next time!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&quot;https://twitter.com/penberg&quot;&gt;Pekka Enberg&lt;/a&gt; for reading a draft of this post!&lt;/em&gt;&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">This week’s paper is from HotOS 2021. Many of the HotOS papers (like this one) propose future directions for Operating Systems research, in addition to including prototypes (in contrast with other papers that focus on a single system’s implementation). The full proceedings of the conference are here - as always, if there are any papers that stick out to you, feel free to reach out on Twitter.</summary></entry><entry><title type="html">Ray: A Distributed Framework for Emerging AI Applications</title><link href="http://www.micahlerner.com/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html" rel="alternate" type="text/html" title="Ray: A Distributed Framework for Emerging AI Applications" /><published>2021-06-27T00:00:00-07:00</published><updated>2021-06-27T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications</id><content type="html" xml:base="http://www.micahlerner.com/2021/06/27/ray-a-distributed-framework-for-emerging-ai-applications.html">&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/osdi18-moritz.pdf&quot;&gt;Ray: A Distributed Framework for Emerging AI Applications&lt;/a&gt; Moritz, Nishihara, Wang et. al.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This week I decided to revisit a paper from the 2018 edition of OSDI (Operating Systems Design and Impelementation). In the coming few weeks, I will likely be reading the &lt;a href=&quot;https://sigops.org/s/conferences/hotos/2021/&quot;&gt;new crop of papers from HotOS 2021&lt;/a&gt;. If there are any that look particuarly exciting to you, feel free to ping me on &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ray is a thriving open-source project focused on &lt;a href=&quot;https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#heading=h.ojukhb92k93n0&quot;&gt;“providing a universal API for distributed computing”&lt;/a&gt; - in other words, trying to build primitives that allow applications to easily run and scale (even across multi-cloud environments), using an actor-like framework. There are a few exciting &lt;a href=&quot;https://www.youtube.com/watch?v=8GTd8Y_JGTQ&quot;&gt;demos&lt;/a&gt; which show how easy it is to parallelize computation&lt;label for=&quot;Anyscale&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;Anyscale&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The demos are from Anyscale, a company founded by several original authors of the paper. Apache Spark is to Databricks, as Ray is to Anyscale. &lt;/span&gt;. The idea that (somewhat) unlimited cloud resources could be used to drastically speed up developer workflows is an exciting area of research - for a specific use case see &lt;a href=&quot;https://stanford.edu/~sadjad/gg-paper.pdf&quot;&gt;From Laptop to Lambda:  Outsourcing Everyday Jobs to Thousands  of Transient Functional Containers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While the current vision of the project has changed from the published paper (which came out of Berkeley’s &lt;a href=&quot;https://rise.cs.berkeley.edu/&quot;&gt;RISELab&lt;/a&gt;&lt;label for=&quot;rise&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;rise&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The RISELab is the &lt;a href=&quot;https://engineering.berkeley.edu/news/2017/01/berkeley-launches-riselab-enabling-computers-to-make-intelligent-real-time-decisions/&quot;&gt;“successor to the AMPLab”&lt;/a&gt;, where Apache Spark, Apache Mesos, and other “big data” technologies were originally developed) &lt;/span&gt;), it is still interesting to reflect on the original architecture and motivation.&lt;/p&gt;

&lt;p&gt;Ray was originally developed with the goal of supporting modern RL applications that must:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Execute large numbers of millisecond-level computations (for example, in response to user requests)&lt;/li&gt;
  &lt;li&gt;Execute workloads on heterogenous resources (running some system workloads on CPUs and others on GPUs)&lt;/li&gt;
  &lt;li&gt;Quickly adapt to new inputs that impact a reinforcement-learning simulation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors argue that existing architectures for RL weren’t able to achieve these goals because of their a-la-carte design - even though technologies existed to solve individual problems associated with running RL models in production, no individual solution was able to cover all of the aforementioned requirements.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;The Ray paper has three main contributions: a generic system designed to train, simulate, and server RL models, the &lt;em&gt;design and architecture&lt;/em&gt; of that system, and a &lt;em&gt;programming model&lt;/em&gt; used to write workloads that run on the system. We will dive into the programming and computation model first, as they are key to understanding the rest of the system.&lt;/p&gt;

&lt;h2 id=&quot;programming-and-computation-model&quot;&gt;Programming and computation model&lt;/h2&gt;

&lt;p&gt;Applications that run on Ray are made up of runnable subcomponents with two types: &lt;em&gt;tasks&lt;/em&gt; or &lt;em&gt;actors&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tasks&lt;/em&gt; are a stateless function execution that rely on their inputs in order to produce a future result (futures are a common abstraction in asynchronous frameworks&lt;label for=&quot;futures&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;futures&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For more on futures, I would recommend &lt;a href=&quot;http://dist-prog-book.com/chapter/2/futures.html&quot;&gt;Heather Miller’s book on  Programming Models for Distributed Computing&lt;/a&gt;. &lt;/span&gt;). A programmer can make a future depend on another future’s result, like one would be able to in most asynch programming frameworks.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Actors&lt;/em&gt; are functions that represent a stateful computation (like a counter), and can depend on or be depended on by other computations. Because they require maintenance of state, they are also more difficult to implement (for example, how is the state recovered in event of failure?).&lt;/p&gt;

&lt;p&gt;Resources can be explicitly allocated to &lt;em&gt;tasks&lt;/em&gt; and &lt;em&gt;actors&lt;/em&gt; - for example, an &lt;em&gt;actor&lt;/em&gt; can be annotated with the number of GPUs it needs.&lt;/p&gt;

&lt;p&gt;Because &lt;em&gt;Tasks&lt;/em&gt; and &lt;em&gt;Actors&lt;/em&gt; in an application can depend on one another, Ray represents their execution as a graph. The nodes in the graph are computation or state that computation produces, while the edges in the graph describe relationships between computations and/or data. Representing computation as a graph allows the state of an application be to re-executed as needed - for example, if part of the state is stored on a node that fails, that state can be recovered &lt;label for=&quot;lineage&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;lineage&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Several of the authors dig further into representing lineage in a future paper &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3341301.3359653&quot;&gt;here&lt;/a&gt;. &lt;/span&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ray/api.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To instatiate &lt;em&gt;tasks&lt;/em&gt; and &lt;em&gt;actors&lt;/em&gt;, Ray provides a developer API in Python (and now in other languages). To initialize a remote function, a developer can add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@ray.remote&lt;/code&gt; decorator. The example below (from the open source project docs &lt;a href=&quot;https://github.com/ray-project/ray#quick-start&quot;&gt;here&lt;/a&gt;) shows how one would create a remote function to square a range of numbers, then wait on the results.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import ray
ray.init()

@ray.remote
def f(x):
    return x * x

futures = [f.remote(i) for i in range(4)]
print(ray.get(futures))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Ray aims to run &lt;em&gt;Tasks&lt;/em&gt; and &lt;em&gt;Actors&lt;/em&gt; created by developers in a fault-tolerant manner. To do so, it implements a distributed system containing two layers: the &lt;em&gt;Application Layer&lt;/em&gt; and the &lt;em&gt;System Layer&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ray/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;the-application-layer&quot;&gt;The Application Layer&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;Application Layer&lt;/em&gt; has three components: a singleton &lt;em&gt;driver&lt;/em&gt; (which orchestrates a specific user program on the cluster), &lt;em&gt;workers&lt;/em&gt; (processes that run tasks), and &lt;em&gt;actors&lt;/em&gt; (which as the name suggests, run &lt;em&gt;Actors&lt;/em&gt; mentioned in the previous section).&lt;/p&gt;

&lt;h3 id=&quot;the-system-layer&quot;&gt;The System Layer&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;System Layer&lt;/em&gt; is significantly more complex, and comprises three components: a &lt;em&gt;Global Control Store&lt;/em&gt; (which maintains state of the system), a &lt;em&gt;scheduler&lt;/em&gt; (which coordinates running computation), and a &lt;em&gt;distributed object store&lt;/em&gt; (which store the input and output of computation).&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Global Control Store&lt;/em&gt; (a.k.a. GCS) is a key-value store that maintains the state of the system. One of its key functions is maintaining the lineage of execution so that the system can recover in the event of failure. The authors argue that separating the system metadata from the scheduler allows every other component of the system to be stateless (making it easier to reason about how to recover if the different subcomponents fail).&lt;/p&gt;

&lt;p&gt;The original paper does not dive into the subcomponents of the &lt;em&gt;GCS&lt;/em&gt;, but the &lt;a href=&quot;https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#&quot;&gt;Ray v1.x Architecture paper&lt;/a&gt; provides more context on how components work (or in some cases, how they have been reworked). One of the most important subystems from the original paper was the &lt;em&gt;Object Table&lt;/em&gt;, which stores locations of values used by Ray operations - for example, on which node a task is storing its output. We will see the &lt;em&gt;Object Table&lt;/em&gt; again in the end-to-end example section of the paper review.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ray/scheduler.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;em&gt;Scheduler&lt;/em&gt; operates “bottoms up” in order to assign the execution of a function to a specific node in the cluster. In contrast to existing schedulers, the Ray scheduler aims to schedule millions of tasks per second (where the tasks are possibly short lived), while also taking into account data locality&lt;label for=&quot;assumptions&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;assumptions&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The paper also mentions other assumptions that existing schedulers make - other schedulers “assume tasks belong to independent jobs, or assume the computation graph is known.” &lt;/span&gt;. Data locality matters for scheduling because the output of computation will end up on a specific node - transferring that data to another node incurs overhead. The scheduler is called “bottoms up” because tasks are first submitted to a local scheduler, only bubbling up to a global scheduler if they cannot be scheduled on the local machine.&lt;/p&gt;

&lt;p&gt;Lastly, the &lt;em&gt;distributed object store&lt;/em&gt; stores immutable inputs and outputs of every task in memory, transferring the inputs for a task to a different machine if needed (for example, if the local scheduler can’t find resources).&lt;/p&gt;

&lt;h3 id=&quot;running-an-application-in-ray&quot;&gt;Running an application in Ray&lt;/h3&gt;

&lt;p&gt;Now that we have an understanding of the different components of Ray, lets walk through an example execution (as described in the original paper)&lt;label for=&quot;caveat&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;caveat&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;I will caveat this section of the paper review with the fact that Ray has changed significantly, and not all functionality may have stayed exactly the same - even so, understanding how the different components fit together is interesting. &lt;/span&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ray/execute.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The execution involves adding the results of two existing Ray tasks (using a call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add.remote(a, b)&lt;/code&gt;). To begin, a function execution is initiated and submitted for scheduling on the local node (&lt;em&gt;N1&lt;/em&gt;). The scheduling request is then forwarded to the &lt;em&gt;Global Scheduler&lt;/em&gt; (possibly because the original node didn’t have enough capacity). The &lt;em&gt;Global Scheduler&lt;/em&gt; checks the &lt;em&gt;Object Table&lt;/em&gt; (which stores task outputs and their locations) for the location of the outputs of the &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; tasks. Then, the &lt;em&gt;Global Scheduler&lt;/em&gt; assigns the new computation to a different node (&lt;em&gt;N2&lt;/em&gt;). &lt;em&gt;N2&lt;/em&gt; only has the outputs of &lt;em&gt;b&lt;/em&gt;, so it needs to fetch the outputs of &lt;em&gt;a&lt;/em&gt; from a remote node. In order to fetch &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;N2&lt;/em&gt; makes a call to the &lt;em&gt;GCS&lt;/em&gt; in order to determine where the other output (&lt;em&gt;a&lt;/em&gt;) is stored, then fetches the output. Lastly, execution begins on &lt;em&gt;N2&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;evaluation-and-microbenchmarks&quot;&gt;Evaluation and microbenchmarks&lt;/h2&gt;

&lt;p&gt;The original paper evaluates whether Ray achieves the desired goal of being able to schedule millions of tasks with variable running times, and whether doing so on heterogenous architecture provides any benefits. A few of the benchmarks stick out to me, primarily those that show how Ray is able to take advantage of heterogenous computing resources.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/ray/ppo.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Ray is primarily impressive in this regard:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ray implementation out-performs the optimized MPI implementation in all experiments, while using a fraction of the GPUs. The reason is that Ray is heterogeneity-aware and allows the user to utilize asymmetric architectures by expressing resource requirements at the granularity of a task or actor. The Ray implementation can then leverage TensorFlow’s single-process multi-GPU support and can pin objects in GPU memory when possible. This optimization cannot be easily ported to MPI due to the need to asynchronously gather rollouts to a single GPU process&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For the Proximal Policy Optimization (PPO) algorithm (more information &lt;a href=&quot;https://openai.com/blog/openai-baselines-ppo/&quot;&gt;on PPO&lt;/a&gt;), the system is able to scale much better than an OpenMPI alternative: “Ray’s fault tolerance and resource-aware scheduling together cut costs by 18×.”&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While originally designed as a system for RL applications, Ray is paving an exciting path forward in computing by providing abstractions on top of cloud resources (in particular, I’m excited to see how the projects innovates in multi-cloud deployments). They have an detailed design document for the new version of the system &lt;a href=&quot;https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you find this paper interesting, &lt;a href=&quot;https://raysummit.anyscale.com/speakers&quot;&gt;Ray Summit&lt;/a&gt; was last week and covers various Ray system internals (in addition to discussions of the technology being adopted in industry).&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Ray: A Distributed Framework for Emerging AI Applications Moritz, Nishihara, Wang et. al.</summary></entry><entry><title type="html">Firecracker: Lightweight Virtualization for Serverless Applications</title><link href="http://www.micahlerner.com/2021/06/17/firecracker-lightweight-virtualization-for-serverless-applications.html" rel="alternate" type="text/html" title="Firecracker: Lightweight Virtualization for Serverless Applications" /><published>2021-06-17T00:00:00-07:00</published><updated>2021-06-17T00:00:00-07:00</updated><id>http://www.micahlerner.com/2021/06/17/firecracker-lightweight-virtualization-for-serverless-applications</id><content type="html" xml:base="http://www.micahlerner.com/2021/06/17/firecracker-lightweight-virtualization-for-serverless-applications.html">&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/conference/nsdi20/presentation/agache&quot;&gt;Firecracker: Lightweight Virtualization for Serverless Applications&lt;/a&gt; Agache et al., NSDI ‘20&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This week’s paper review is a bit different than the past few weeks (which have been about distributed key-value stores). Inspired by all of the neat projects being built with the technology discussed in this paper, I decided to learn more. Enjoy!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Firecracker is a high-performance virtualization solution built to run Amazon’s serverless&lt;label for=&quot;serverless&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;serverless&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Serverless meaning that the resources for running a workload are provided on-demand, rather than being paid for over a prolonged time-period. Martin Fowler has some great docs on the topic &lt;a href=&quot;https://martinfowler.com/articles/serverless.html&quot;&gt;here&lt;/a&gt;. &lt;/span&gt; applications securely and with minimal resources. It now does so at immense scale (at the time the paper was published, it supported “millions of production workloads, and trillions of requests per month”).&lt;/p&gt;

&lt;p&gt;Since the paper was published, there has a been a buzz of interesting projects built with Firecracker. &lt;a href=&quot;https://fly.io&quot;&gt;Fly.io&lt;/a&gt; (a speed-focused platform for running Docker applications&lt;label for=&quot;fly&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;fly&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Apologies if a Fly.io engineer reads this and has a different short summary of the company. I did my best. &lt;/span&gt;) wrote about using the technology on their &lt;a href=&quot;https://fly.io/blog/sandboxing-and-workload-isolation/&quot;&gt;blog&lt;/a&gt;, &lt;a href=&quot;https://jvns.ca&quot;&gt;Julia Evans&lt;/a&gt; wrote about booting them up for a &lt;a href=&quot;https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/&quot;&gt;CTF she was building&lt;/a&gt;, and &lt;a href=&quot;https://github.com/weaveworks/ignite&quot;&gt;Weave Ingite&lt;/a&gt; lets you launch virtual machines from Docker&lt;label for=&quot;vms&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;vms&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Virtual machines and containers are sometimes conflated to be one and the same, but the internals are different. The difference is discussed later in this paper review! :) &lt;/span&gt; containers (and other &lt;a href=&quot;https://opencontainers.org/&quot;&gt;OCI&lt;/a&gt;&lt;label for=&quot;oci&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;oci&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;OCI stands for “Open Container Initiative” and works to define standards for containers and the software that runs them. A nice thing about OCI containers is that you can run them (with a container runtime) that complies with the standards, but has different internals. For example, one could choose &lt;a href=&quot;https://podman.io/&quot;&gt;Podman&lt;/a&gt; instead of Docker. &lt;/span&gt; images).&lt;/p&gt;

&lt;p&gt;Now that you are excited about Firecracker, let’s jump into the paper!&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-papers-contributions&quot;&gt;What are the paper’s contributions?&lt;/h2&gt;

&lt;p&gt;There are two main contributions from the paper: the Firecracker system itself (already discussed above), and the usage of Firecracker to power AWS Lambda (Amazon’s platform for running serverless workloads).&lt;/p&gt;

&lt;p&gt;Before we go further, it is important to understand the motivation behind building Firecracker in the first place.&lt;/p&gt;

&lt;p&gt;Originally, Lambda functions ran on a separate virtual machine (VM) for every customer (although functions from the same customer would run in the same VM). Allocating a separate VM for every customer was great for isolating customers from each other - you wouldn’t want Company A to access Company B’s code or functionality, nor for Company A’s greedy resource consumption to starve Company B’s Lambdas of resources.&lt;/p&gt;

&lt;p&gt;Unfortunately, existing VM solutions required significant resources, and resulted in non-optimal utilization. For example, a customer might have a VM allocated to them, but the VM is not frequently used. Even though the VM isn’t used to its full capacity, there is still memory and CPU being consumed to run the VM. The Lambda system in this form was less-efficient, meaning it required more resources to scale (likely making the system more expensive for customers).&lt;/p&gt;

&lt;p&gt;With the goal of increasing utilization (and lowering cost), the team established constraints of a possible future solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Overhead and density&lt;/em&gt;: Run “thousands of functions on a single machine, with minimal waste”. In other words, solving one of the main problems of the existing architecture.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Isolation&lt;/em&gt;: Ensure that applications are completely separate from one another (can’t read each other’s data, nor learn about them through side channels). The existing solution had this property, but at high cost.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Performance&lt;/em&gt;: A new solution should have the same or better performance as before.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Compatibility&lt;/em&gt;: Run any binary “without code changes or recompilation”. &lt;label for=&quot;compat&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;compat&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;This requirement was there, even though Lambda oringally supported a small set of languages. Making a generic solution was planning for the long-term! &lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Fast Switching&lt;/em&gt;: “It must be possible to start new functions and clean up old functions quickly”.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Soft Allocation&lt;/em&gt;: “It must be possible to over commit CPU, memory, and other resources”. This requirement impacts utilization (and in turn, the cost of the system to AWS/the customer). Overcommittment comes into play a few times during a Firecracker VM’s lifetime. For example, when it starts up, it theoretically is allocated resources, but may not be using them right away if it is performing set up work. Other times, the VM may need to burst above the configured soft-limit on resources, and would need to consume those of another VM. The paper note’s “We have tested memory and CPU oversubscription ratios of over 20x, and run in production with ratios as high as 10x, with no issues” - very neat!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The constraints were applied to three different categories of solutions: &lt;em&gt;Linux containers&lt;/em&gt;, &lt;em&gt;language-specific isolation&lt;/em&gt;, and &lt;em&gt;alternative virtualization solutions&lt;/em&gt; (they were already using virtualization, but wanted to consider a different option than their existing implementation).&lt;/p&gt;

&lt;h3 id=&quot;linux-containers&quot;&gt;Linux containers&lt;/h3&gt;

&lt;p&gt;There are several &lt;em&gt;Isolation&lt;/em&gt; downsides to using Linux containers.&lt;/p&gt;

&lt;p&gt;First, Linux containers interact directly with a host OS using syscalls&lt;label for=&quot;syscalls&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;syscalls&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Syscalls are a standard way for programs to interact with an operating system. They’re really neat. I highly reccommend &lt;a href=&quot;http://beej.us/guide/bgnet/&quot;&gt;Beej’s guide to Network Programming&lt;/a&gt; for some fun syscall programming &lt;/span&gt;. One can lock-down which syscalls a program can make (the paper mentions using &lt;a href=&quot;https://www.kernel.org/doc/html/v4.16/userspace-api/seccomp_filter.html&quot;&gt;Seccomp BPF&lt;/a&gt;), and even which arguments the syscalls can use, as well as using other security features of container systems (the Fly.io article linked above discusses this topic in more depth).&lt;/p&gt;

&lt;p&gt;Even using other Linux isolation features, at the end of the day the container is still interacting with the OS. That means that if customer code in the container figures out a way to pwn the OS, or figures out a side channel to determine state of another container, &lt;em&gt;Isolation&lt;/em&gt; might break down. Not great.&lt;/p&gt;

&lt;h3 id=&quot;language-specific-isolation&quot;&gt;Language-specific isolation&lt;/h3&gt;

&lt;p&gt;While there are ways to run language-specific VMs (like the JVM for Java/Scala/Clojure or V8 for Javascript), this approach doesn’t scale well to many different languages (nor does it allow for a system that can run arbitrary binaries - one of the original design goals).&lt;/p&gt;

&lt;h3 id=&quot;alternative-virtualization-solutions&quot;&gt;Alternative Virtualization Solutions&lt;/h3&gt;

&lt;p&gt;Revisiting virtualization led to a focus on what about the existing virtualization approach was holding Lambda back:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Isolation&lt;/em&gt;: the code associated with the components of virtualization are lengthy (meaning more possible areas of exploitation), and &lt;a href=&quot;https://www.computerworld.com/article/3182877/pwn2own-ends-with-two-virtual-machine-escapes.html&quot;&gt;researchers have escaped from virtual machines before&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Overhead and density&lt;/em&gt;: the components of virtualization (which we will get into further down) require too many resources, leading to low utilization&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Fast switching&lt;/em&gt;: VMs take a while to boot and shut down, which doesn’t mesh well with Lambda functions that need a VM quickly and may only use it for a few seconds (or less).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The team then applied the above requirements to the main components of the virtualization system: the hypervisor and the virtual machine monitor.&lt;/p&gt;

&lt;p&gt;First, the team considered which &lt;em&gt;type&lt;/em&gt; of hypervisor to choose. There are two types of hypervisors, Type 1 and Type 2. The textbook definitions of hypervisors say that Type 1 hypervisors are integrated directly in the hardware, while Type 2 hypervisors run an operating system on top of the hardware (then run the hypervisor on top of that operating system).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/Hypervisor.svg&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;Type 1 vs Type 2 Hypervisors. Scsami, CC0, via Wikimedia Commons&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Linux has a robust hypervisor built into the kernel, called &lt;a href=&quot;https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf&quot;&gt;Kernel Virtual Machine&lt;/a&gt; (a.k.a. KVM) that is arguably a Type 1 hypervisor&lt;label for=&quot;type1&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;type1&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;a href=&quot;https://serverfault.com/questions/855094/is-kvm-a-type-1-or-type-2-hypervisor&quot;&gt;Different resources&lt;/a&gt; make &lt;a href=&quot;https://virtualizationreview.com/Blogs/Mental-Ward/2009/02/KVM-BareMetal-Hypervisor.aspx&quot;&gt;different arguments&lt;/a&gt; for whether KVM is a Type 1 or Type 2 hypervisor. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Using a hypervisor like KVM allows for kernel components to be moved into userspace - if the kernel components are in user space and they get pwned, the host OS itself hasn’t been pwned. Linux provides an interface, &lt;a href=&quot;https://wiki.libvirt.org/page/Virtio&quot;&gt;virtio&lt;/a&gt;&lt;label for=&quot;virtio&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;virtio&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Fun fact: the author of the paper on virtio, Rusty Russell, is now a key developer of a main &lt;a href=&quot;https://github.com/ElementsProject/lightning&quot;&gt;Bitcoin Lightning implementation&lt;/a&gt;. &lt;/span&gt;, that allows the user space kernel components to interact with the host OS. Rather than passing all interactions with a guest kernel directly to the host kernel, some functions, in particular device interactions, go from a guest kernel to a &lt;em&gt;virtual machine monitor&lt;/em&gt; (a.k.a. VMM). One of the most popular VMMs is &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usenix05/tech/freenix/full_papers/bellard/bellard.pdf&quot;&gt;QEMU&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/virt.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Unfortunately, QEMU has a significant amount of code (again, more code means more potential attack surface), as it supports a full range of functionality - even functionality that a Lambda would never use, like USB drivers. Rather than trying to pare down QEMU, the team forked &lt;a href=&quot;https://opensource.google/projects/crosvm&quot;&gt;crosvm&lt;/a&gt;&lt;label for=&quot;crosvmfork&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;crosvmfork&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;I enjoyed &lt;a href=&quot;https://prilik.com/blog/post/crosvm-paravirt/&quot;&gt;this&lt;/a&gt; post on crosvm from a former Google intern. &lt;/span&gt; (a VMM open-sourced by Google, and developed for ChromeOS), in the process significantly rewriting core functionality for Firecracker’s use case. The end result was a slimmer library with only code that would conceivably be used by a Lambda - resulting in 50k lines of Rust (versus &amp;gt; 1.4 million lines of C in QEMU&lt;label for=&quot;QEMU&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;QEMU&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Relatedly, there was an interesting &lt;a href=&quot;http://blog.vmsplice.net/2020/08/why-qemu-should-move-from-c-to-rust.html&quot;&gt;blog post&lt;/a&gt; about QEMU security issues and thoughts on Rust from a QEMU maintainer. &lt;/span&gt;). Because the goal of Firecracker is to be as small as possible, the paper calls the project a &lt;em&gt;MicroVM&lt;/em&gt;, rather than “VM”.&lt;/p&gt;

&lt;h2 id=&quot;how-do-firecracker-microvms-get-run-on-aws&quot;&gt;How do Firecracker MicroVMs get run on AWS?&lt;/h2&gt;

&lt;p&gt;Now that we roughly understand how Firecracker works, let’s dive into how it is used in running Lambda. First, we will look at how the Lambda architecture works on a high level, followed by a look at how the running the Lambda itself works.&lt;/p&gt;

&lt;h3 id=&quot;high-level-architecture-of-aws-lambda&quot;&gt;High-level architecture of AWS Lambda&lt;/h3&gt;

&lt;p&gt;When a developer runs (or &lt;em&gt;Invokes&lt;/em&gt;, in AWS terminology) a Lambda, the ensuing HTTP request hits an AWS Load Balancer &lt;label for=&quot;aws&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;aws&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Lambdas can also start via other events - like ‘integrations with other AWS services including storage (S3), queue (SQS), streaming data (Kinesis) and database (DynamoDB) services.’ &lt;/span&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/arch.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;There are a four main infrastructure components involved in running a Lambda once it has been invoked:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Workers&lt;/em&gt;: The components that actually run a Lambda’s code. Each worker runs many MicroVMs in “slots”, and other services schedule code to be run in the MicroVMs when a customer &lt;em&gt;Invokes&lt;/em&gt; a Lambda.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Frontend&lt;/em&gt;: The entrance into the Lambda system. It receives &lt;em&gt;Invoke&lt;/em&gt; requests, and communicates with the  &lt;em&gt;Worker Manager&lt;/em&gt; to determine where to run the Lambda, then directly communicates with the &lt;em&gt;Workers&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Worker Manager&lt;/em&gt;: Ensures that the same Lambda is routed to the same set of &lt;em&gt;Workers&lt;/em&gt; (this routing impacts performance for reasons that we will learn more about in the next section). It keeps tracks of where a Lambda has been scheduled previously. These previous runs correspond to “slots” for a function. If all of the slots for a function are in use, the &lt;em&gt;Worker Manager&lt;/em&gt; works with the &lt;em&gt;Placement&lt;/em&gt; service to find more slots in the &lt;em&gt;Workers&lt;/em&gt; fleet.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Placement&lt;/em&gt; service: Makes scheduling decisions when it needs to assign a Lambda invocation to a &lt;em&gt;Worker&lt;/em&gt;. It makes these decision in order to “optimize the placement of slots for a single function across the worker fleet, ensuring that the utilization of resources including CPU, memory, network, and storage is even across the fleet and the potential for correlated resource allocation on each individual worker is minimized”.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lambda-worker-architecture&quot;&gt;Lambda worker architecture&lt;/h3&gt;

&lt;p&gt;Each Lambda worker has thousands of individual &lt;em&gt;MicroVMs&lt;/em&gt; that map to a “slot”.&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/lambdaworker.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Each MicroVM is associated with resource constraints (configured when a Lambda is setup) and communicates with several components that allow for scheduling, isolated execution, and teardown of customer code inside of a Lambda:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Firecracker VM&lt;/em&gt;: All of the goodness we talked about earlier.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Shim process&lt;/em&gt;: A process inside of the VM that communicates with an external side car called the &lt;em&gt;Micro Manager&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Micro Manager&lt;/em&gt;: a sidecar that communicates over TCP with a &lt;em&gt;Shim process&lt;/em&gt; running inside the VM. It reports metadata that it receives back to the &lt;em&gt;Placement&lt;/em&gt; service, and can be called by the &lt;em&gt;Frontend&lt;/em&gt; in order to &lt;em&gt;Invoke&lt;/em&gt; a specific function. On function completion, the &lt;em&gt;Micro Manager&lt;/em&gt; also receives the response from the &lt;em&gt;Shim process&lt;/em&gt; running inside the VM (passing it back to the client as needed).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While slots can be filled on demand, the &lt;em&gt;Micro Manager&lt;/em&gt; also starts up Firecracker VMs in advance - this helps with performance (as we will see in the next section).&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;Firecracker was evaluated relative to similar VMM solutions on three dimensions: &lt;em&gt;boot times&lt;/em&gt;, &lt;em&gt;memory overhead&lt;/em&gt;, and &lt;em&gt;IO Performance&lt;/em&gt;. In these tests, Firecracker was compared to QEMU and Intel Cloud Hypervisor&lt;label for=&quot;crosvm&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;crosvm&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Interestingly, Firecracker wasn’t compared to crosvm. I am not sure if this is because it wasn’t possible, or whether the authors of the paper thought it wouldn’t be a fair comparison. &lt;/span&gt;. Additionally, there are two configurations of Firecracker used in the tests: Firecracker and Firecracker-pre. Because Firecracker MicroVMs are configured via API calls, the team tested setups where the API calls had completed (Firecracker-pre, where the “pre” means “pre-configured”) or had not completed (regular Firecracker). The timer for both of these configurations ended when the &lt;em&gt;init&lt;/em&gt; process in the VM started.&lt;/p&gt;

&lt;h3 id=&quot;boot-times&quot;&gt;Boot times&lt;/h3&gt;

&lt;p&gt;The boot time comparisons involved two configurations: booting 500 total MicroVMs serially, and booting 1000 total MicroVMs, 50 at a time (in parallel).&lt;/p&gt;

&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/boot_time.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The bottom line from these tests is that Firecracker MicroVMs boot incredibly quickly - &lt;em&gt;Fast switching&lt;/em&gt; ✅ !&lt;/p&gt;

&lt;h3 id=&quot;memory-overhead&quot;&gt;Memory overhead&lt;/h3&gt;

&lt;p&gt;Relative to the other options, Firecracker uses significantly less memory - &lt;em&gt;overhead and density&lt;/em&gt; ✅!&lt;/p&gt;
&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/mem.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;io-performance&quot;&gt;IO Performance&lt;/h3&gt;

&lt;p&gt;Relative to the other options, Firecracker and the comparable solution of Intel’s Cloud Hypervisor didn’t perform well in all tests. The paper argues that the causes of relatively inferior performance in the IO tests are no flushing to disk and an implementation of block IOs that performs IO serially - the paper notes that “we expect to fix these limitations with time”. Digging into Github issues for Firecracker, I &lt;a href=&quot;https://github.com/firecracker-microvm/firecracker/issues/1600&quot;&gt;found one&lt;/a&gt; that indicates they were prototyping use of &lt;a href=&quot;https://unixism.net/loti/what_is_io_uring.html&quot;&gt;io_uring&lt;/a&gt; to support async IO (and increase IO performance).&lt;/p&gt;
&lt;figure&gt;&lt;img class=&quot;maincolumn-img&quot; src=&quot;/assets/firecracker/io.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Firecracker was interesting to learn about because it is a high-performance, low overhead VMM written in Rust. The paper also is a great study in pragmatic technical decision making - rather than rewriting already robust software (KVM), the team focused on a specific component of an existing system to improve. Along the way, we learned about how different methods for &lt;em&gt;isolating&lt;/em&gt; customer workloads from each other &lt;label for=&quot;bpf&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;bpf&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;In particular, I thought seccomp-bpf was interesting and look forward to learning more about BPF/eBPF. First stop: &lt;a href=&quot;https://jvns.ca/blog/2017/06/28/notes-on-bpf---ebpf/&quot;&gt;Julia Evans’ guide&lt;/a&gt; &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;If you made it this far, you probably enjoyed the paper review - I post them on my &lt;a href=&quot;https://twitter.com/micahlerner&quot;&gt;Twitter&lt;/a&gt; every week!&lt;/p&gt;</content><author><name>Micah</name></author><summary type="html">Firecracker: Lightweight Virtualization for Serverless Applications Agache et al., NSDI ‘20</summary></entry></feed>